{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wines Points prediction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submission Date : 3.6.2023\n",
    "Task: Predict the wine score given the inputs\n",
    "Instructions:\n",
    " * Use logistic regression as benchmark model\n",
    " * Use sklearn pipeliens + cv + grid search with sklearn models (e.g. KNNs, RandomForest, etc.)\n",
    " * Compare all models on proper metric (your choice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For DNN course project:\n",
    "* Use sklearn pipeliens with tensorflow models (w/wo embeddings, LSTMs, RNNs, Transformers etc.)\n",
    "* Compare all models on proper metric (your choice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys; sys.path.append('../')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will try to predict the points a wine will get based on known characteristics (i.e. features, in the ML terminology). The mine point in this stage is to establish a simple, ideally super cost effective, basline.\n",
    "In the real world there is a tradeoff between complexity and perforamnce, and the DS job, among others, is to present a tradeoff tables of what performance is achivalbel at what complexity level. \n",
    "\n",
    "to which models with increased complexity and resource demands will be compared. Complexity should then be translated into cost. For example:\n",
    " * Compute cost \n",
    " * Maintenance cost\n",
    " * Serving costs (i.e. is new platform needed?) \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.9.0.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import cufflinks as cf; cf.go_offline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_reviews = pd.read_csv(\"clean_wine_reviews_data.csv\")\n",
    "summary_df = pd.read_csv(\"summary_df.csv\")\n",
    "wine_reviews.shape\n",
    "\n",
    "wine_reviews['description_length'] = wine_reviews.description.str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>country</th>\n",
       "      <th>description</th>\n",
       "      <th>designation</th>\n",
       "      <th>points</th>\n",
       "      <th>price</th>\n",
       "      <th>province</th>\n",
       "      <th>region_1</th>\n",
       "      <th>region_2</th>\n",
       "      <th>taster_name</th>\n",
       "      <th>taster_twitter_handle</th>\n",
       "      <th>title</th>\n",
       "      <th>variety</th>\n",
       "      <th>winery</th>\n",
       "      <th>year</th>\n",
       "      <th>wine_category</th>\n",
       "      <th>desc_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Italy</td>\n",
       "      <td>Aromas include tropical fruit broom brimstone ...</td>\n",
       "      <td>Vulkà Bianco</td>\n",
       "      <td>87</td>\n",
       "      <td>39.928286</td>\n",
       "      <td>Sicily &amp; Sardinia</td>\n",
       "      <td>Etna</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Kerin O’Keefe</td>\n",
       "      <td>@kerinokeefe</td>\n",
       "      <td>Nicosia 2013 Vulkà Bianco  (Etna)</td>\n",
       "      <td>White Blend</td>\n",
       "      <td>Nicosia</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>White</td>\n",
       "      <td>152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Portugal</td>\n",
       "      <td>This ripe fruity wine smooth still structured ...</td>\n",
       "      <td>Avidagos</td>\n",
       "      <td>87</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>Douro</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Roger Voss</td>\n",
       "      <td>@vossroger</td>\n",
       "      <td>Quinta dos Avidagos 2011 Avidagos Red (Douro)</td>\n",
       "      <td>Portuguese Red</td>\n",
       "      <td>Quinta dos Avidagos</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>Red</td>\n",
       "      <td>160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>US</td>\n",
       "      <td>Tart snappy flavors lime flesh rind dominate S...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>87</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>Willamette Valley</td>\n",
       "      <td>Willamette Valley</td>\n",
       "      <td>Paul Gregutt</td>\n",
       "      <td>@paulgwine</td>\n",
       "      <td>Rainstorm 2013 Pinot Gris (Willamette Valley)</td>\n",
       "      <td>Pinot Gris</td>\n",
       "      <td>Rainstorm</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>White</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>US</td>\n",
       "      <td>Pineapple rind lemon pith orange blossom start...</td>\n",
       "      <td>Reserve Late Harvest</td>\n",
       "      <td>87</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>Michigan</td>\n",
       "      <td>Lake Michigan Shore</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Alexander Peartree</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>St. Julian 2013 Reserve Late Harvest Riesling ...</td>\n",
       "      <td>Riesling</td>\n",
       "      <td>St. Julian</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>White</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>US</td>\n",
       "      <td>Much like regular bottling 2012 comes across r...</td>\n",
       "      <td>Vintner's Reserve Wild Child Block</td>\n",
       "      <td>87</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>Willamette Valley</td>\n",
       "      <td>Willamette Valley</td>\n",
       "      <td>Paul Gregutt</td>\n",
       "      <td>@paulgwine</td>\n",
       "      <td>Sweet Cheeks 2012 Vintner's Reserve Wild Child...</td>\n",
       "      <td>Pinot Noir</td>\n",
       "      <td>Sweet Cheeks</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>Red</td>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0   country                                        description  \\\n",
       "0           0     Italy  Aromas include tropical fruit broom brimstone ...   \n",
       "1           1  Portugal  This ripe fruity wine smooth still structured ...   \n",
       "2           2        US  Tart snappy flavors lime flesh rind dominate S...   \n",
       "3           3        US  Pineapple rind lemon pith orange blossom start...   \n",
       "4           4        US  Much like regular bottling 2012 comes across r...   \n",
       "\n",
       "                          designation  points      price           province  \\\n",
       "0                        Vulkà Bianco      87  39.928286  Sicily & Sardinia   \n",
       "1                            Avidagos      87  15.000000              Douro   \n",
       "2                             Unknown      87  14.000000             Oregon   \n",
       "3                Reserve Late Harvest      87  13.000000           Michigan   \n",
       "4  Vintner's Reserve Wild Child Block      87  65.000000             Oregon   \n",
       "\n",
       "              region_1           region_2         taster_name  \\\n",
       "0                 Etna            Unknown       Kerin O’Keefe   \n",
       "1              Unknown            Unknown          Roger Voss   \n",
       "2    Willamette Valley  Willamette Valley        Paul Gregutt   \n",
       "3  Lake Michigan Shore            Unknown  Alexander Peartree   \n",
       "4    Willamette Valley  Willamette Valley        Paul Gregutt   \n",
       "\n",
       "  taster_twitter_handle                                              title  \\\n",
       "0          @kerinokeefe                  Nicosia 2013 Vulkà Bianco  (Etna)   \n",
       "1            @vossroger      Quinta dos Avidagos 2011 Avidagos Red (Douro)   \n",
       "2           @paulgwine       Rainstorm 2013 Pinot Gris (Willamette Valley)   \n",
       "3               Unknown  St. Julian 2013 Reserve Late Harvest Riesling ...   \n",
       "4           @paulgwine   Sweet Cheeks 2012 Vintner's Reserve Wild Child...   \n",
       "\n",
       "          variety               winery    year wine_category  desc_len  \n",
       "0     White Blend              Nicosia  2013.0         White       152  \n",
       "1  Portuguese Red  Quinta dos Avidagos  2011.0           Red       160  \n",
       "2      Pinot Gris            Rainstorm  2013.0         White       149  \n",
       "3        Riesling           St. Julian  2013.0         White       155  \n",
       "4      Pinot Noir         Sweet Cheeks  2012.0           Red       185  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine_reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding, Concatenate, Flatten, TextVectorization, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import MeanSquaredError\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical, plot_model, pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import activations\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting Model 1, Parameters: {'desc_words': 1000, 'desc_len': 626, 'activation': 'relu', 'units layer 1': 8, 'units layer 2': 4}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 1000, 'desc_len': 626, 'activation': 'relu', 'units layer 1': 8, 'units layer 2': 4}\n",
      "2624/2624 - 4s - loss: 3920.1030 - 4s/epoch - 2ms/step\n",
      "875/875 - 1s - loss: 3920.4141 - 1s/epoch - 2ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 1000, 'desc_len': 626, 'activation': 'relu', 'units layer 1': 8, 'units layer 2': 8}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 1000, 'desc_len': 626, 'activation': 'relu', 'units layer 1': 8, 'units layer 2': 8}\n",
      "2624/2624 - 4s - loss: 1434.7241 - 4s/epoch - 2ms/step\n",
      "875/875 - 1s - loss: 1435.3904 - 1s/epoch - 2ms/step\n",
      "Passing Parameters: {'desc_words': 1000, 'desc_len': 626, 'activation': 'relu', 'units layer 1': 8, 'units layer 2': 16}\n",
      "Passing Parameters: {'desc_words': 1000, 'desc_len': 626, 'activation': 'relu', 'units layer 1': 8, 'units layer 2': 32}\n",
      "Passing Parameters: {'desc_words': 1000, 'desc_len': 626, 'activation': 'relu', 'units layer 1': 8, 'units layer 2': 64}\n",
      "Fitting Model 1, Parameters: {'desc_words': 1000, 'desc_len': 626, 'activation': 'relu', 'units layer 1': 16, 'units layer 2': 4}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 1000, 'desc_len': 626, 'activation': 'relu', 'units layer 1': 16, 'units layer 2': 4}\n",
      "2624/2624 - 5s - loss: 794.2374 - 5s/epoch - 2ms/step\n",
      "875/875 - 1s - loss: 794.7361 - 1s/epoch - 2ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 1000, 'desc_len': 626, 'activation': 'relu', 'units layer 1': 16, 'units layer 2': 8}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 1000, 'desc_len': 626, 'activation': 'relu', 'units layer 1': 16, 'units layer 2': 8}\n",
      "2624/2624 - 5s - loss: 1068.5568 - 5s/epoch - 2ms/step\n",
      "875/875 - 2s - loss: 1069.0422 - 2s/epoch - 2ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 1000, 'desc_len': 626, 'activation': 'relu', 'units layer 1': 16, 'units layer 2': 16}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 1000, 'desc_len': 626, 'activation': 'relu', 'units layer 1': 16, 'units layer 2': 16}\n",
      "2624/2624 - 4s - loss: 1195.9038 - 4s/epoch - 2ms/step\n",
      "875/875 - 1s - loss: 1196.5095 - 1s/epoch - 2ms/step\n",
      "Passing Parameters: {'desc_words': 1000, 'desc_len': 626, 'activation': 'relu', 'units layer 1': 16, 'units layer 2': 32}\n",
      "Passing Parameters: {'desc_words': 1000, 'desc_len': 626, 'activation': 'relu', 'units layer 1': 16, 'units layer 2': 64}\n",
      "Fitting Model 1, Parameters: {'desc_words': 1000, 'desc_len': 626, 'activation': 'relu', 'units layer 1': 32, 'units layer 2': 4}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 1000, 'desc_len': 626, 'activation': 'relu', 'units layer 1': 32, 'units layer 2': 4}\n",
      "2624/2624 - 5s - loss: 1160.7494 - 5s/epoch - 2ms/step\n",
      "875/875 - 2s - loss: 1161.1333 - 2s/epoch - 2ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 1000, 'desc_len': 626, 'activation': 'relu', 'units layer 1': 32, 'units layer 2': 8}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 1000, 'desc_len': 626, 'activation': 'relu', 'units layer 1': 32, 'units layer 2': 8}\n",
      "2624/2624 - 5s - loss: 544.2765 - 5s/epoch - 2ms/step\n",
      "875/875 - 1s - loss: 544.6620 - 1s/epoch - 2ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 1000, 'desc_len': 626, 'activation': 'relu', 'units layer 1': 32, 'units layer 2': 16}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 1000, 'desc_len': 626, 'activation': 'relu', 'units layer 1': 32, 'units layer 2': 16}\n",
      "2624/2624 - 5s - loss: 1245.4436 - 5s/epoch - 2ms/step\n",
      "875/875 - 2s - loss: 1246.0179 - 2s/epoch - 2ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 1000, 'desc_len': 626, 'activation': 'relu', 'units layer 1': 32, 'units layer 2': 32}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 1000, 'desc_len': 626, 'activation': 'relu', 'units layer 1': 32, 'units layer 2': 32}\n",
      "2624/2624 - 5s - loss: 1616.6329 - 5s/epoch - 2ms/step\n",
      "875/875 - 2s - loss: 1617.0852 - 2s/epoch - 2ms/step\n",
      "Passing Parameters: {'desc_words': 1000, 'desc_len': 626, 'activation': 'relu', 'units layer 1': 32, 'units layer 2': 64}\n",
      "Fitting Model 1, Parameters: {'desc_words': 1000, 'desc_len': 626, 'activation': 'relu', 'units layer 1': 64, 'units layer 2': 4}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 1000, 'desc_len': 626, 'activation': 'relu', 'units layer 1': 64, 'units layer 2': 4}\n",
      "2624/2624 - 6s - loss: 1176.9200 - 6s/epoch - 2ms/step\n",
      "875/875 - 2s - loss: 1177.0439 - 2s/epoch - 2ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 1000, 'desc_len': 626, 'activation': 'relu', 'units layer 1': 64, 'units layer 2': 8}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 1000, 'desc_len': 626, 'activation': 'relu', 'units layer 1': 64, 'units layer 2': 8}\n",
      "2624/2624 - 6s - loss: 1721.4093 - 6s/epoch - 2ms/step\n",
      "875/875 - 2s - loss: 1721.9309 - 2s/epoch - 2ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 1000, 'desc_len': 626, 'activation': 'relu', 'units layer 1': 64, 'units layer 2': 16}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 1000, 'desc_len': 626, 'activation': 'relu', 'units layer 1': 64, 'units layer 2': 16}\n",
      "2624/2624 - 6s - loss: 1375.0831 - 6s/epoch - 2ms/step\n",
      "875/875 - 2s - loss: 1375.5015 - 2s/epoch - 2ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 1000, 'desc_len': 626, 'activation': 'relu', 'units layer 1': 64, 'units layer 2': 32}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 1000, 'desc_len': 626, 'activation': 'relu', 'units layer 1': 64, 'units layer 2': 32}\n",
      "2624/2624 - 6s - loss: 1747.1685 - 6s/epoch - 2ms/step\n",
      "875/875 - 2s - loss: 1747.7733 - 2s/epoch - 2ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 1000, 'desc_len': 626, 'activation': 'relu', 'units layer 1': 64, 'units layer 2': 64}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 1000, 'desc_len': 626, 'activation': 'relu', 'units layer 1': 64, 'units layer 2': 64}\n",
      "2624/2624 - 6s - loss: 1716.7477 - 6s/epoch - 2ms/step\n",
      "875/875 - 2s - loss: 1717.3892 - 2s/epoch - 2ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 1000, 'desc_len': 626, 'activation': 'relu', 'units layer 1': 128, 'units layer 2': 4}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 1000, 'desc_len': 626, 'activation': 'relu', 'units layer 1': 128, 'units layer 2': 4}\n",
      "2624/2624 - 7s - loss: 907.6434 - 7s/epoch - 3ms/step\n",
      "875/875 - 2s - loss: 907.9134 - 2s/epoch - 3ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 1000, 'desc_len': 626, 'activation': 'relu', 'units layer 1': 128, 'units layer 2': 8}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 1000, 'desc_len': 626, 'activation': 'relu', 'units layer 1': 128, 'units layer 2': 8}\n",
      "2624/2624 - 7s - loss: 1270.2721 - 7s/epoch - 3ms/step\n",
      "875/875 - 2s - loss: 1270.3822 - 2s/epoch - 3ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 1000, 'desc_len': 626, 'activation': 'relu', 'units layer 1': 128, 'units layer 2': 16}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 1000, 'desc_len': 626, 'activation': 'relu', 'units layer 1': 128, 'units layer 2': 16}\n",
      "2624/2624 - 7s - loss: 1797.9261 - 7s/epoch - 3ms/step\n",
      "875/875 - 2s - loss: 1798.2678 - 2s/epoch - 2ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 1000, 'desc_len': 626, 'activation': 'relu', 'units layer 1': 128, 'units layer 2': 32}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 1000, 'desc_len': 626, 'activation': 'relu', 'units layer 1': 128, 'units layer 2': 32}\n",
      "2624/2624 - 7s - loss: 1827.8706 - 7s/epoch - 3ms/step\n",
      "875/875 - 2s - loss: 1828.4185 - 2s/epoch - 2ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 1000, 'desc_len': 626, 'activation': 'relu', 'units layer 1': 128, 'units layer 2': 64}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 1000, 'desc_len': 626, 'activation': 'relu', 'units layer 1': 128, 'units layer 2': 64}\n",
      "2624/2624 - 6s - loss: 1890.2986 - 6s/epoch - 2ms/step\n",
      "875/875 - 2s - loss: 1890.6895 - 2s/epoch - 3ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 1000, 'desc_len': 626, 'activation': 'sigmoid', 'units layer 1': 8, 'units layer 2': 4}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 1000, 'desc_len': 626, 'activation': 'sigmoid', 'units layer 1': 8, 'units layer 2': 4}\n",
      "2624/2624 - 4s - loss: 9.1735 - 4s/epoch - 2ms/step\n",
      "875/875 - 1s - loss: 9.2699 - 1s/epoch - 2ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 1000, 'desc_len': 626, 'activation': 'sigmoid', 'units layer 1': 8, 'units layer 2': 8}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 1000, 'desc_len': 626, 'activation': 'sigmoid', 'units layer 1': 8, 'units layer 2': 8}\n",
      "2624/2624 - 5s - loss: 9.1748 - 5s/epoch - 2ms/step\n",
      "875/875 - 2s - loss: 9.2713 - 2s/epoch - 2ms/step\n",
      "Passing Parameters: {'desc_words': 1000, 'desc_len': 626, 'activation': 'sigmoid', 'units layer 1': 8, 'units layer 2': 16}\n",
      "Passing Parameters: {'desc_words': 1000, 'desc_len': 626, 'activation': 'sigmoid', 'units layer 1': 8, 'units layer 2': 32}\n",
      "Passing Parameters: {'desc_words': 1000, 'desc_len': 626, 'activation': 'sigmoid', 'units layer 1': 8, 'units layer 2': 64}\n",
      "Fitting Model 1, Parameters: {'desc_words': 1000, 'desc_len': 626, 'activation': 'sigmoid', 'units layer 1': 16, 'units layer 2': 4}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 1000, 'desc_len': 626, 'activation': 'sigmoid', 'units layer 1': 16, 'units layer 2': 4}\n",
      "2624/2624 - 4s - loss: 9.1732 - 4s/epoch - 2ms/step\n",
      "875/875 - 2s - loss: 9.2697 - 2s/epoch - 2ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 1000, 'desc_len': 626, 'activation': 'sigmoid', 'units layer 1': 16, 'units layer 2': 8}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 1000, 'desc_len': 626, 'activation': 'sigmoid', 'units layer 1': 16, 'units layer 2': 8}\n",
      "2624/2624 - 4s - loss: 9.1780 - 4s/epoch - 2ms/step\n",
      "875/875 - 2s - loss: 9.2741 - 2s/epoch - 2ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 1000, 'desc_len': 626, 'activation': 'sigmoid', 'units layer 1': 16, 'units layer 2': 16}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 1000, 'desc_len': 626, 'activation': 'sigmoid', 'units layer 1': 16, 'units layer 2': 16}\n",
      "2624/2624 - 4s - loss: 9.1794 - 4s/epoch - 2ms/step\n",
      "875/875 - 1s - loss: 9.2755 - 1s/epoch - 2ms/step\n",
      "Passing Parameters: {'desc_words': 1000, 'desc_len': 626, 'activation': 'sigmoid', 'units layer 1': 16, 'units layer 2': 32}\n",
      "Passing Parameters: {'desc_words': 1000, 'desc_len': 626, 'activation': 'sigmoid', 'units layer 1': 16, 'units layer 2': 64}\n",
      "Fitting Model 1, Parameters: {'desc_words': 1000, 'desc_len': 626, 'activation': 'sigmoid', 'units layer 1': 32, 'units layer 2': 4}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 1000, 'desc_len': 626, 'activation': 'sigmoid', 'units layer 1': 32, 'units layer 2': 4}\n",
      "2624/2624 - 6s - loss: 9.1792 - 6s/epoch - 2ms/step\n",
      "875/875 - 2s - loss: 9.2753 - 2s/epoch - 2ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 1000, 'desc_len': 626, 'activation': 'sigmoid', 'units layer 1': 32, 'units layer 2': 8}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 1000, 'desc_len': 626, 'activation': 'sigmoid', 'units layer 1': 32, 'units layer 2': 8}\n",
      "2624/2624 - 6s - loss: 9.1734 - 6s/epoch - 2ms/step\n",
      "875/875 - 2s - loss: 9.2699 - 2s/epoch - 2ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 1000, 'desc_len': 626, 'activation': 'sigmoid', 'units layer 1': 32, 'units layer 2': 16}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 1000, 'desc_len': 626, 'activation': 'sigmoid', 'units layer 1': 32, 'units layer 2': 16}\n",
      "2624/2624 - 5s - loss: 9.1833 - 5s/epoch - 2ms/step\n",
      "875/875 - 2s - loss: 9.2800 - 2s/epoch - 2ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 1000, 'desc_len': 626, 'activation': 'sigmoid', 'units layer 1': 32, 'units layer 2': 32}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 1000, 'desc_len': 626, 'activation': 'sigmoid', 'units layer 1': 32, 'units layer 2': 32}\n",
      "2624/2624 - 5s - loss: 2.9759 - 5s/epoch - 2ms/step\n",
      "875/875 - 2s - loss: 3.2773 - 2s/epoch - 2ms/step\n",
      "Passing Parameters: {'desc_words': 1000, 'desc_len': 626, 'activation': 'sigmoid', 'units layer 1': 32, 'units layer 2': 64}\n",
      "Fitting Model 1, Parameters: {'desc_words': 1000, 'desc_len': 626, 'activation': 'sigmoid', 'units layer 1': 64, 'units layer 2': 4}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 1000, 'desc_len': 626, 'activation': 'sigmoid', 'units layer 1': 64, 'units layer 2': 4}\n",
      "2624/2624 - 6s - loss: 114.3933 - 6s/epoch - 2ms/step\n",
      "875/875 - 2s - loss: 114.5252 - 2s/epoch - 2ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 1000, 'desc_len': 626, 'activation': 'sigmoid', 'units layer 1': 64, 'units layer 2': 8}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 1000, 'desc_len': 626, 'activation': 'sigmoid', 'units layer 1': 64, 'units layer 2': 8}\n",
      "2624/2624 - 6s - loss: 9.1756 - 6s/epoch - 2ms/step\n",
      "875/875 - 2s - loss: 9.2722 - 2s/epoch - 2ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 1000, 'desc_len': 626, 'activation': 'sigmoid', 'units layer 1': 64, 'units layer 2': 16}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 1000, 'desc_len': 626, 'activation': 'sigmoid', 'units layer 1': 64, 'units layer 2': 16}\n",
      "2624/2624 - 6s - loss: 9.1791 - 6s/epoch - 2ms/step\n",
      "875/875 - 2s - loss: 9.2758 - 2s/epoch - 2ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 1000, 'desc_len': 626, 'activation': 'sigmoid', 'units layer 1': 64, 'units layer 2': 32}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 1000, 'desc_len': 626, 'activation': 'sigmoid', 'units layer 1': 64, 'units layer 2': 32}\n",
      "2624/2624 - 6s - loss: 3.1088 - 6s/epoch - 2ms/step\n",
      "875/875 - 2s - loss: 3.3137 - 2s/epoch - 2ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 1000, 'desc_len': 626, 'activation': 'sigmoid', 'units layer 1': 64, 'units layer 2': 64}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 1000, 'desc_len': 626, 'activation': 'sigmoid', 'units layer 1': 64, 'units layer 2': 64}\n",
      "2624/2624 - 6s - loss: 9.1840 - 6s/epoch - 2ms/step\n",
      "875/875 - 2s - loss: 9.2801 - 2s/epoch - 2ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 1000, 'desc_len': 626, 'activation': 'sigmoid', 'units layer 1': 128, 'units layer 2': 4}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 1000, 'desc_len': 626, 'activation': 'sigmoid', 'units layer 1': 128, 'units layer 2': 4}\n",
      "2624/2624 - 9s - loss: 215.1145 - 9s/epoch - 3ms/step\n",
      "875/875 - 6s - loss: 215.2605 - 6s/epoch - 7ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 1000, 'desc_len': 626, 'activation': 'sigmoid', 'units layer 1': 128, 'units layer 2': 8}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 1000, 'desc_len': 626, 'activation': 'sigmoid', 'units layer 1': 128, 'units layer 2': 8}\n",
      "2624/2624 - 16s - loss: 9.1759 - 16s/epoch - 6ms/step\n",
      "875/875 - 4s - loss: 9.2725 - 4s/epoch - 5ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 1000, 'desc_len': 626, 'activation': 'sigmoid', 'units layer 1': 128, 'units layer 2': 16}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 1000, 'desc_len': 626, 'activation': 'sigmoid', 'units layer 1': 128, 'units layer 2': 16}\n",
      "2624/2624 - 15s - loss: 3.1989 - 15s/epoch - 6ms/step\n",
      "875/875 - 5s - loss: 3.3974 - 5s/epoch - 5ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 1000, 'desc_len': 626, 'activation': 'sigmoid', 'units layer 1': 128, 'units layer 2': 32}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 1000, 'desc_len': 626, 'activation': 'sigmoid', 'units layer 1': 128, 'units layer 2': 32}\n",
      "2624/2624 - 14s - loss: 9.1742 - 14s/epoch - 5ms/step\n",
      "875/875 - 5s - loss: 9.2706 - 5s/epoch - 6ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 1000, 'desc_len': 626, 'activation': 'sigmoid', 'units layer 1': 128, 'units layer 2': 64}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 1000, 'desc_len': 626, 'activation': 'sigmoid', 'units layer 1': 128, 'units layer 2': 64}\n",
      "2624/2624 - 14s - loss: 9.2015 - 14s/epoch - 5ms/step\n",
      "875/875 - 4s - loss: 9.2985 - 4s/epoch - 5ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 1000, 'desc_len': 60, 'activation': 'relu', 'units layer 1': 8, 'units layer 2': 4}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 1000, 'desc_len': 60, 'activation': 'relu', 'units layer 1': 8, 'units layer 2': 4}\n",
      "2624/2624 - 3s - loss: 1203.0024 - 3s/epoch - 1ms/step\n",
      "875/875 - 1s - loss: 1203.5544 - 1s/epoch - 1ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 1000, 'desc_len': 60, 'activation': 'relu', 'units layer 1': 8, 'units layer 2': 8}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 1000, 'desc_len': 60, 'activation': 'relu', 'units layer 1': 8, 'units layer 2': 8}\n",
      "2624/2624 - 3s - loss: 823.3359 - 3s/epoch - 1ms/step\n",
      "875/875 - 1s - loss: 823.7498 - 1s/epoch - 1ms/step\n",
      "Passing Parameters: {'desc_words': 1000, 'desc_len': 60, 'activation': 'relu', 'units layer 1': 8, 'units layer 2': 16}\n",
      "Passing Parameters: {'desc_words': 1000, 'desc_len': 60, 'activation': 'relu', 'units layer 1': 8, 'units layer 2': 32}\n",
      "Passing Parameters: {'desc_words': 1000, 'desc_len': 60, 'activation': 'relu', 'units layer 1': 8, 'units layer 2': 64}\n",
      "Fitting Model 1, Parameters: {'desc_words': 1000, 'desc_len': 60, 'activation': 'relu', 'units layer 1': 16, 'units layer 2': 4}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 1000, 'desc_len': 60, 'activation': 'relu', 'units layer 1': 16, 'units layer 2': 4}\n",
      "2624/2624 - 3s - loss: 716.9431 - 3s/epoch - 1ms/step\n",
      "875/875 - 1s - loss: 717.3441 - 1s/epoch - 1ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 1000, 'desc_len': 60, 'activation': 'relu', 'units layer 1': 16, 'units layer 2': 8}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 1000, 'desc_len': 60, 'activation': 'relu', 'units layer 1': 16, 'units layer 2': 8}\n",
      "2624/2624 - 4s - loss: 1880.8828 - 4s/epoch - 2ms/step\n",
      "875/875 - 1s - loss: 1881.2909 - 1s/epoch - 2ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 1000, 'desc_len': 60, 'activation': 'relu', 'units layer 1': 16, 'units layer 2': 16}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 1000, 'desc_len': 60, 'activation': 'relu', 'units layer 1': 16, 'units layer 2': 16}\n",
      "2624/2624 - 3s - loss: 1241.6752 - 3s/epoch - 1ms/step\n",
      "875/875 - 1s - loss: 1242.0933 - 1s/epoch - 1ms/step\n",
      "Passing Parameters: {'desc_words': 1000, 'desc_len': 60, 'activation': 'relu', 'units layer 1': 16, 'units layer 2': 32}\n",
      "Passing Parameters: {'desc_words': 1000, 'desc_len': 60, 'activation': 'relu', 'units layer 1': 16, 'units layer 2': 64}\n",
      "Fitting Model 1, Parameters: {'desc_words': 1000, 'desc_len': 60, 'activation': 'relu', 'units layer 1': 32, 'units layer 2': 4}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 1000, 'desc_len': 60, 'activation': 'relu', 'units layer 1': 32, 'units layer 2': 4}\n",
      "2624/2624 - 4s - loss: 2.9195 - 4s/epoch - 2ms/step\n",
      "875/875 - 1s - loss: 3.3775 - 1s/epoch - 2ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 1000, 'desc_len': 60, 'activation': 'relu', 'units layer 1': 32, 'units layer 2': 8}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 1000, 'desc_len': 60, 'activation': 'relu', 'units layer 1': 32, 'units layer 2': 8}\n",
      "2624/2624 - 4s - loss: 1617.4528 - 4s/epoch - 2ms/step\n",
      "875/875 - 1s - loss: 1618.1633 - 1s/epoch - 1ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 1000, 'desc_len': 60, 'activation': 'relu', 'units layer 1': 32, 'units layer 2': 16}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 1000, 'desc_len': 60, 'activation': 'relu', 'units layer 1': 32, 'units layer 2': 16}\n",
      "2624/2624 - 4s - loss: 1889.3015 - 4s/epoch - 1ms/step\n",
      "875/875 - 1s - loss: 1889.7096 - 1s/epoch - 1ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 1000, 'desc_len': 60, 'activation': 'relu', 'units layer 1': 32, 'units layer 2': 32}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 1000, 'desc_len': 60, 'activation': 'relu', 'units layer 1': 32, 'units layer 2': 32}\n",
      "2624/2624 - 4s - loss: 1459.0223 - 4s/epoch - 1ms/step\n",
      "875/875 - 1s - loss: 1459.5629 - 1s/epoch - 1ms/step\n",
      "Passing Parameters: {'desc_words': 1000, 'desc_len': 60, 'activation': 'relu', 'units layer 1': 32, 'units layer 2': 64}\n",
      "Fitting Model 1, Parameters: {'desc_words': 1000, 'desc_len': 60, 'activation': 'relu', 'units layer 1': 64, 'units layer 2': 4}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 1000, 'desc_len': 60, 'activation': 'relu', 'units layer 1': 64, 'units layer 2': 4}\n",
      "2624/2624 - 4s - loss: 7.7482 - 4s/epoch - 2ms/step\n",
      "875/875 - 2s - loss: 7.9218 - 2s/epoch - 2ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 1000, 'desc_len': 60, 'activation': 'relu', 'units layer 1': 64, 'units layer 2': 8}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 1000, 'desc_len': 60, 'activation': 'relu', 'units layer 1': 64, 'units layer 2': 8}\n",
      "2624/2624 - 4s - loss: 8.7332 - 4s/epoch - 2ms/step\n",
      "875/875 - 2s - loss: 8.8840 - 2s/epoch - 2ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 1000, 'desc_len': 60, 'activation': 'relu', 'units layer 1': 64, 'units layer 2': 16}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 1000, 'desc_len': 60, 'activation': 'relu', 'units layer 1': 64, 'units layer 2': 16}\n",
      "2624/2624 - 4s - loss: 931.2863 - 4s/epoch - 2ms/step\n",
      "875/875 - 2s - loss: 931.7635 - 2s/epoch - 2ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 1000, 'desc_len': 60, 'activation': 'relu', 'units layer 1': 64, 'units layer 2': 32}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 1000, 'desc_len': 60, 'activation': 'relu', 'units layer 1': 64, 'units layer 2': 32}\n",
      "2624/2624 - 4s - loss: 1504.4470 - 4s/epoch - 2ms/step\n",
      "875/875 - 2s - loss: 1504.8921 - 2s/epoch - 2ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 1000, 'desc_len': 60, 'activation': 'relu', 'units layer 1': 64, 'units layer 2': 64}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 1000, 'desc_len': 60, 'activation': 'relu', 'units layer 1': 64, 'units layer 2': 64}\n",
      "2624/2624 - 5s - loss: 851.9370 - 5s/epoch - 2ms/step\n",
      "875/875 - 1s - loss: 852.4502 - 1s/epoch - 2ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 1000, 'desc_len': 60, 'activation': 'relu', 'units layer 1': 128, 'units layer 2': 4}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 1000, 'desc_len': 60, 'activation': 'relu', 'units layer 1': 128, 'units layer 2': 4}\n",
      "2624/2624 - 5s - loss: 375.3934 - 5s/epoch - 2ms/step\n",
      "875/875 - 1s - loss: 375.4504 - 1s/epoch - 2ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 1000, 'desc_len': 60, 'activation': 'relu', 'units layer 1': 128, 'units layer 2': 8}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 1000, 'desc_len': 60, 'activation': 'relu', 'units layer 1': 128, 'units layer 2': 8}\n",
      "2624/2624 - 5s - loss: 284.5070 - 5s/epoch - 2ms/step\n",
      "875/875 - 2s - loss: 284.7483 - 2s/epoch - 2ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 1000, 'desc_len': 60, 'activation': 'relu', 'units layer 1': 128, 'units layer 2': 16}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 1000, 'desc_len': 60, 'activation': 'relu', 'units layer 1': 128, 'units layer 2': 16}\n",
      "2624/2624 - 4s - loss: 2.8965 - 4s/epoch - 2ms/step\n",
      "875/875 - 2s - loss: 3.4035 - 2s/epoch - 2ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 1000, 'desc_len': 60, 'activation': 'relu', 'units layer 1': 128, 'units layer 2': 32}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 1000, 'desc_len': 60, 'activation': 'relu', 'units layer 1': 128, 'units layer 2': 32}\n",
      "2624/2624 - 4s - loss: 45.7025 - 4s/epoch - 2ms/step\n",
      "875/875 - 2s - loss: 45.9252 - 2s/epoch - 2ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 1000, 'desc_len': 60, 'activation': 'relu', 'units layer 1': 128, 'units layer 2': 64}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 1000, 'desc_len': 60, 'activation': 'relu', 'units layer 1': 128, 'units layer 2': 64}\n",
      "2624/2624 - 5s - loss: 1379.8877 - 5s/epoch - 2ms/step\n",
      "875/875 - 1s - loss: 1380.4137 - 1s/epoch - 2ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 1000, 'desc_len': 60, 'activation': 'sigmoid', 'units layer 1': 8, 'units layer 2': 4}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 1000, 'desc_len': 60, 'activation': 'sigmoid', 'units layer 1': 8, 'units layer 2': 4}\n",
      "2624/2624 - 3s - loss: 9.1739 - 3s/epoch - 1ms/step\n",
      "875/875 - 1s - loss: 9.2702 - 973ms/epoch - 1ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 1000, 'desc_len': 60, 'activation': 'sigmoid', 'units layer 1': 8, 'units layer 2': 8}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 1000, 'desc_len': 60, 'activation': 'sigmoid', 'units layer 1': 8, 'units layer 2': 8}\n",
      "2624/2624 - 3s - loss: 9.1745 - 3s/epoch - 1ms/step\n",
      "875/875 - 1s - loss: 9.2708 - 1s/epoch - 1ms/step\n",
      "Passing Parameters: {'desc_words': 1000, 'desc_len': 60, 'activation': 'sigmoid', 'units layer 1': 8, 'units layer 2': 16}\n",
      "Passing Parameters: {'desc_words': 1000, 'desc_len': 60, 'activation': 'sigmoid', 'units layer 1': 8, 'units layer 2': 32}\n",
      "Passing Parameters: {'desc_words': 1000, 'desc_len': 60, 'activation': 'sigmoid', 'units layer 1': 8, 'units layer 2': 64}\n",
      "Fitting Model 1, Parameters: {'desc_words': 1000, 'desc_len': 60, 'activation': 'sigmoid', 'units layer 1': 16, 'units layer 2': 4}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 1000, 'desc_len': 60, 'activation': 'sigmoid', 'units layer 1': 16, 'units layer 2': 4}\n",
      "2624/2624 - 5s - loss: 9.1736 - 5s/epoch - 2ms/step\n",
      "875/875 - 1s - loss: 9.2699 - 1s/epoch - 1ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 1000, 'desc_len': 60, 'activation': 'sigmoid', 'units layer 1': 16, 'units layer 2': 8}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 1000, 'desc_len': 60, 'activation': 'sigmoid', 'units layer 1': 16, 'units layer 2': 8}\n",
      "2624/2624 - 4s - loss: 9.1797 - 4s/epoch - 1ms/step\n",
      "875/875 - 2s - loss: 9.2759 - 2s/epoch - 2ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 1000, 'desc_len': 60, 'activation': 'sigmoid', 'units layer 1': 16, 'units layer 2': 16}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 1000, 'desc_len': 60, 'activation': 'sigmoid', 'units layer 1': 16, 'units layer 2': 16}\n",
      "2624/2624 - 4s - loss: 9.1775 - 4s/epoch - 2ms/step\n",
      "875/875 - 1s - loss: 9.2736 - 1s/epoch - 1ms/step\n",
      "Passing Parameters: {'desc_words': 1000, 'desc_len': 60, 'activation': 'sigmoid', 'units layer 1': 16, 'units layer 2': 32}\n",
      "Passing Parameters: {'desc_words': 1000, 'desc_len': 60, 'activation': 'sigmoid', 'units layer 1': 16, 'units layer 2': 64}\n",
      "Fitting Model 1, Parameters: {'desc_words': 1000, 'desc_len': 60, 'activation': 'sigmoid', 'units layer 1': 32, 'units layer 2': 4}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 1000, 'desc_len': 60, 'activation': 'sigmoid', 'units layer 1': 32, 'units layer 2': 4}\n",
      "2624/2624 - 5s - loss: 9.1733 - 5s/epoch - 2ms/step\n",
      "875/875 - 1s - loss: 9.2698 - 1s/epoch - 2ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 1000, 'desc_len': 60, 'activation': 'sigmoid', 'units layer 1': 32, 'units layer 2': 8}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 1000, 'desc_len': 60, 'activation': 'sigmoid', 'units layer 1': 32, 'units layer 2': 8}\n",
      "2624/2624 - 5s - loss: 3.1416 - 5s/epoch - 2ms/step\n",
      "875/875 - 1s - loss: 3.3477 - 1s/epoch - 2ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 1000, 'desc_len': 60, 'activation': 'sigmoid', 'units layer 1': 32, 'units layer 2': 16}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 1000, 'desc_len': 60, 'activation': 'sigmoid', 'units layer 1': 32, 'units layer 2': 16}\n",
      "2624/2624 - 5s - loss: 3.2119 - 5s/epoch - 2ms/step\n",
      "875/875 - 2s - loss: 3.3801 - 2s/epoch - 2ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 1000, 'desc_len': 60, 'activation': 'sigmoid', 'units layer 1': 32, 'units layer 2': 32}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 1000, 'desc_len': 60, 'activation': 'sigmoid', 'units layer 1': 32, 'units layer 2': 32}\n",
      "2624/2624 - 4s - loss: 9.1747 - 4s/epoch - 2ms/step\n",
      "875/875 - 1s - loss: 9.2712 - 1s/epoch - 2ms/step\n",
      "Passing Parameters: {'desc_words': 1000, 'desc_len': 60, 'activation': 'sigmoid', 'units layer 1': 32, 'units layer 2': 64}\n",
      "Fitting Model 1, Parameters: {'desc_words': 1000, 'desc_len': 60, 'activation': 'sigmoid', 'units layer 1': 64, 'units layer 2': 4}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 1000, 'desc_len': 60, 'activation': 'sigmoid', 'units layer 1': 64, 'units layer 2': 4}\n",
      "2624/2624 - 5s - loss: 9.2405 - 5s/epoch - 2ms/step\n",
      "875/875 - 2s - loss: 9.3378 - 2s/epoch - 2ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 1000, 'desc_len': 60, 'activation': 'sigmoid', 'units layer 1': 64, 'units layer 2': 8}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 1000, 'desc_len': 60, 'activation': 'sigmoid', 'units layer 1': 64, 'units layer 2': 8}\n",
      "2624/2624 - 5s - loss: 9.1733 - 5s/epoch - 2ms/step\n",
      "875/875 - 2s - loss: 9.2697 - 2s/epoch - 2ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 1000, 'desc_len': 60, 'activation': 'sigmoid', 'units layer 1': 64, 'units layer 2': 16}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 1000, 'desc_len': 60, 'activation': 'sigmoid', 'units layer 1': 64, 'units layer 2': 16}\n",
      "2624/2624 - 5s - loss: 4.3107 - 5s/epoch - 2ms/step\n",
      "875/875 - 2s - loss: 4.4104 - 2s/epoch - 2ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 1000, 'desc_len': 60, 'activation': 'sigmoid', 'units layer 1': 64, 'units layer 2': 32}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 1000, 'desc_len': 60, 'activation': 'sigmoid', 'units layer 1': 64, 'units layer 2': 32}\n",
      "2624/2624 - 6s - loss: 9.1775 - 6s/epoch - 2ms/step\n",
      "875/875 - 2s - loss: 9.2737 - 2s/epoch - 2ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 1000, 'desc_len': 60, 'activation': 'sigmoid', 'units layer 1': 64, 'units layer 2': 64}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 1000, 'desc_len': 60, 'activation': 'sigmoid', 'units layer 1': 64, 'units layer 2': 64}\n",
      "2624/2624 - 5s - loss: 9.2336 - 5s/epoch - 2ms/step\n",
      "875/875 - 2s - loss: 9.3291 - 2s/epoch - 2ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 1000, 'desc_len': 60, 'activation': 'sigmoid', 'units layer 1': 128, 'units layer 2': 4}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 1000, 'desc_len': 60, 'activation': 'sigmoid', 'units layer 1': 128, 'units layer 2': 4}\n",
      "2624/2624 - 5s - loss: 3439.3442 - 5s/epoch - 2ms/step\n",
      "875/875 - 2s - loss: 3439.6455 - 2s/epoch - 2ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 1000, 'desc_len': 60, 'activation': 'sigmoid', 'units layer 1': 128, 'units layer 2': 8}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 1000, 'desc_len': 60, 'activation': 'sigmoid', 'units layer 1': 128, 'units layer 2': 8}\n",
      "2624/2624 - 5s - loss: 9.1733 - 5s/epoch - 2ms/step\n",
      "875/875 - 2s - loss: 9.2697 - 2s/epoch - 2ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 1000, 'desc_len': 60, 'activation': 'sigmoid', 'units layer 1': 128, 'units layer 2': 16}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 1000, 'desc_len': 60, 'activation': 'sigmoid', 'units layer 1': 128, 'units layer 2': 16}\n",
      "2624/2624 - 5s - loss: 3.2715 - 5s/epoch - 2ms/step\n",
      "875/875 - 2s - loss: 3.4199 - 2s/epoch - 2ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 1000, 'desc_len': 60, 'activation': 'sigmoid', 'units layer 1': 128, 'units layer 2': 32}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 1000, 'desc_len': 60, 'activation': 'sigmoid', 'units layer 1': 128, 'units layer 2': 32}\n",
      "2624/2624 - 5s - loss: 9.1743 - 5s/epoch - 2ms/step\n",
      "875/875 - 2s - loss: 9.2706 - 2s/epoch - 2ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 1000, 'desc_len': 60, 'activation': 'sigmoid', 'units layer 1': 128, 'units layer 2': 64}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 1000, 'desc_len': 60, 'activation': 'sigmoid', 'units layer 1': 128, 'units layer 2': 64}\n",
      "2624/2624 - 5s - loss: 9.2229 - 5s/epoch - 2ms/step\n",
      "875/875 - 2s - loss: 9.3185 - 2s/epoch - 2ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 5000, 'desc_len': 626, 'activation': 'relu', 'units layer 1': 8, 'units layer 2': 4}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 5000, 'desc_len': 626, 'activation': 'relu', 'units layer 1': 8, 'units layer 2': 4}\n",
      "2624/2624 - 9s - loss: 1102.6815 - 9s/epoch - 4ms/step\n",
      "875/875 - 4s - loss: 1103.6083 - 4s/epoch - 5ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 5000, 'desc_len': 626, 'activation': 'relu', 'units layer 1': 8, 'units layer 2': 8}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 5000, 'desc_len': 626, 'activation': 'relu', 'units layer 1': 8, 'units layer 2': 8}\n",
      "2624/2624 - 7s - loss: 9.1740 - 7s/epoch - 3ms/step\n",
      "875/875 - 2s - loss: 9.2706 - 2s/epoch - 2ms/step\n",
      "Passing Parameters: {'desc_words': 5000, 'desc_len': 626, 'activation': 'relu', 'units layer 1': 8, 'units layer 2': 16}\n",
      "Passing Parameters: {'desc_words': 5000, 'desc_len': 626, 'activation': 'relu', 'units layer 1': 8, 'units layer 2': 32}\n",
      "Passing Parameters: {'desc_words': 5000, 'desc_len': 626, 'activation': 'relu', 'units layer 1': 8, 'units layer 2': 64}\n",
      "Fitting Model 1, Parameters: {'desc_words': 5000, 'desc_len': 626, 'activation': 'relu', 'units layer 1': 16, 'units layer 2': 4}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 5000, 'desc_len': 626, 'activation': 'relu', 'units layer 1': 16, 'units layer 2': 4}\n",
      "2624/2624 - 6s - loss: 256.7925 - 6s/epoch - 2ms/step\n",
      "875/875 - 2s - loss: 257.0373 - 2s/epoch - 2ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 5000, 'desc_len': 626, 'activation': 'relu', 'units layer 1': 16, 'units layer 2': 8}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 5000, 'desc_len': 626, 'activation': 'relu', 'units layer 1': 16, 'units layer 2': 8}\n",
      "2624/2624 - 5s - loss: 1629.2667 - 5s/epoch - 2ms/step\n",
      "875/875 - 2s - loss: 1630.2964 - 2s/epoch - 2ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 5000, 'desc_len': 626, 'activation': 'relu', 'units layer 1': 16, 'units layer 2': 16}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 5000, 'desc_len': 626, 'activation': 'relu', 'units layer 1': 16, 'units layer 2': 16}\n",
      "2624/2624 - 7s - loss: 942.3215 - 7s/epoch - 3ms/step\n",
      "875/875 - 3s - loss: 943.4494 - 3s/epoch - 3ms/step\n",
      "Passing Parameters: {'desc_words': 5000, 'desc_len': 626, 'activation': 'relu', 'units layer 1': 16, 'units layer 2': 32}\n",
      "Passing Parameters: {'desc_words': 5000, 'desc_len': 626, 'activation': 'relu', 'units layer 1': 16, 'units layer 2': 64}\n",
      "Fitting Model 1, Parameters: {'desc_words': 5000, 'desc_len': 626, 'activation': 'relu', 'units layer 1': 32, 'units layer 2': 4}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 5000, 'desc_len': 626, 'activation': 'relu', 'units layer 1': 32, 'units layer 2': 4}\n",
      "2624/2624 - 7s - loss: 3.0358 - 7s/epoch - 3ms/step\n",
      "875/875 - 3s - loss: 3.6246 - 3s/epoch - 3ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 5000, 'desc_len': 626, 'activation': 'relu', 'units layer 1': 32, 'units layer 2': 8}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 5000, 'desc_len': 626, 'activation': 'relu', 'units layer 1': 32, 'units layer 2': 8}\n",
      "2624/2624 - 7s - loss: 3.3048 - 7s/epoch - 3ms/step\n",
      "875/875 - 3s - loss: 4.2900 - 3s/epoch - 3ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 5000, 'desc_len': 626, 'activation': 'relu', 'units layer 1': 32, 'units layer 2': 16}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 5000, 'desc_len': 626, 'activation': 'relu', 'units layer 1': 32, 'units layer 2': 16}\n",
      "2624/2624 - 7s - loss: 1552.8481 - 7s/epoch - 3ms/step\n",
      "875/875 - 3s - loss: 1553.6248 - 3s/epoch - 3ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 5000, 'desc_len': 626, 'activation': 'relu', 'units layer 1': 32, 'units layer 2': 32}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 5000, 'desc_len': 626, 'activation': 'relu', 'units layer 1': 32, 'units layer 2': 32}\n",
      "2624/2624 - 7s - loss: 1610.3573 - 7s/epoch - 3ms/step\n",
      "875/875 - 3s - loss: 1611.4185 - 3s/epoch - 3ms/step\n",
      "Passing Parameters: {'desc_words': 5000, 'desc_len': 626, 'activation': 'relu', 'units layer 1': 32, 'units layer 2': 64}\n",
      "Fitting Model 1, Parameters: {'desc_words': 5000, 'desc_len': 626, 'activation': 'relu', 'units layer 1': 64, 'units layer 2': 4}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 5000, 'desc_len': 626, 'activation': 'relu', 'units layer 1': 64, 'units layer 2': 4}\n",
      "2624/2624 - 16s - loss: 282.8033 - 16s/epoch - 6ms/step\n",
      "875/875 - 7s - loss: 283.1772 - 7s/epoch - 8ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 5000, 'desc_len': 626, 'activation': 'relu', 'units layer 1': 64, 'units layer 2': 8}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 5000, 'desc_len': 626, 'activation': 'relu', 'units layer 1': 64, 'units layer 2': 8}\n",
      "2624/2624 - 14s - loss: 1701.3389 - 14s/epoch - 5ms/step\n",
      "875/875 - 4s - loss: 1701.8425 - 4s/epoch - 5ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 5000, 'desc_len': 626, 'activation': 'relu', 'units layer 1': 64, 'units layer 2': 16}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 5000, 'desc_len': 626, 'activation': 'relu', 'units layer 1': 64, 'units layer 2': 16}\n",
      "2624/2624 - 11s - loss: 1564.9205 - 11s/epoch - 4ms/step\n",
      "875/875 - 4s - loss: 1565.9519 - 4s/epoch - 4ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 5000, 'desc_len': 626, 'activation': 'relu', 'units layer 1': 64, 'units layer 2': 32}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 5000, 'desc_len': 626, 'activation': 'relu', 'units layer 1': 64, 'units layer 2': 32}\n",
      "2624/2624 - 13s - loss: 1762.6378 - 13s/epoch - 5ms/step\n",
      "875/875 - 4s - loss: 1763.2285 - 4s/epoch - 5ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 5000, 'desc_len': 626, 'activation': 'relu', 'units layer 1': 64, 'units layer 2': 64}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 5000, 'desc_len': 626, 'activation': 'relu', 'units layer 1': 64, 'units layer 2': 64}\n",
      "2624/2624 - 12s - loss: 1755.6465 - 12s/epoch - 5ms/step\n",
      "875/875 - 4s - loss: 1756.7915 - 4s/epoch - 5ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 5000, 'desc_len': 626, 'activation': 'relu', 'units layer 1': 128, 'units layer 2': 4}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 5000, 'desc_len': 626, 'activation': 'relu', 'units layer 1': 128, 'units layer 2': 4}\n",
      "2624/2624 - 15s - loss: 350.3645 - 15s/epoch - 6ms/step\n",
      "875/875 - 5s - loss: 350.9300 - 5s/epoch - 5ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 5000, 'desc_len': 626, 'activation': 'relu', 'units layer 1': 128, 'units layer 2': 8}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 5000, 'desc_len': 626, 'activation': 'relu', 'units layer 1': 128, 'units layer 2': 8}\n",
      "2624/2624 - 15s - loss: 1785.3424 - 15s/epoch - 6ms/step\n",
      "875/875 - 5s - loss: 1785.7307 - 5s/epoch - 5ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 5000, 'desc_len': 626, 'activation': 'relu', 'units layer 1': 128, 'units layer 2': 16}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 5000, 'desc_len': 626, 'activation': 'relu', 'units layer 1': 128, 'units layer 2': 16}\n",
      "2624/2624 - 15s - loss: 1776.9583 - 15s/epoch - 6ms/step\n",
      "875/875 - 6s - loss: 1777.4126 - 6s/epoch - 7ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 5000, 'desc_len': 626, 'activation': 'relu', 'units layer 1': 128, 'units layer 2': 32}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 5000, 'desc_len': 626, 'activation': 'relu', 'units layer 1': 128, 'units layer 2': 32}\n",
      "2624/2624 - 15s - loss: 2218.9307 - 15s/epoch - 6ms/step\n",
      "875/875 - 4s - loss: 2219.3486 - 4s/epoch - 5ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 5000, 'desc_len': 626, 'activation': 'relu', 'units layer 1': 128, 'units layer 2': 64}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 5000, 'desc_len': 626, 'activation': 'relu', 'units layer 1': 128, 'units layer 2': 64}\n",
      "2624/2624 - 14s - loss: 1800.3982 - 14s/epoch - 5ms/step\n",
      "875/875 - 5s - loss: 1801.3118 - 5s/epoch - 6ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 5000, 'desc_len': 626, 'activation': 'sigmoid', 'units layer 1': 8, 'units layer 2': 4}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 5000, 'desc_len': 626, 'activation': 'sigmoid', 'units layer 1': 8, 'units layer 2': 4}\n",
      "2624/2624 - 9s - loss: 9.1738 - 9s/epoch - 3ms/step\n",
      "875/875 - 2s - loss: 9.2701 - 2s/epoch - 3ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 5000, 'desc_len': 626, 'activation': 'sigmoid', 'units layer 1': 8, 'units layer 2': 8}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 5000, 'desc_len': 626, 'activation': 'sigmoid', 'units layer 1': 8, 'units layer 2': 8}\n",
      "2624/2624 - 8s - loss: 9.1779 - 8s/epoch - 3ms/step\n",
      "875/875 - 2s - loss: 9.2741 - 2s/epoch - 2ms/step\n",
      "Passing Parameters: {'desc_words': 5000, 'desc_len': 626, 'activation': 'sigmoid', 'units layer 1': 8, 'units layer 2': 16}\n",
      "Passing Parameters: {'desc_words': 5000, 'desc_len': 626, 'activation': 'sigmoid', 'units layer 1': 8, 'units layer 2': 32}\n",
      "Passing Parameters: {'desc_words': 5000, 'desc_len': 626, 'activation': 'sigmoid', 'units layer 1': 8, 'units layer 2': 64}\n",
      "Fitting Model 1, Parameters: {'desc_words': 5000, 'desc_len': 626, 'activation': 'sigmoid', 'units layer 1': 16, 'units layer 2': 4}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 5000, 'desc_len': 626, 'activation': 'sigmoid', 'units layer 1': 16, 'units layer 2': 4}\n",
      "2624/2624 - 7s - loss: 9.1739 - 7s/epoch - 3ms/step\n",
      "875/875 - 3s - loss: 9.2703 - 3s/epoch - 3ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 5000, 'desc_len': 626, 'activation': 'sigmoid', 'units layer 1': 16, 'units layer 2': 8}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 5000, 'desc_len': 626, 'activation': 'sigmoid', 'units layer 1': 16, 'units layer 2': 8}\n",
      "2624/2624 - 8s - loss: 9.1735 - 8s/epoch - 3ms/step\n",
      "875/875 - 2s - loss: 9.2699 - 2s/epoch - 3ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 5000, 'desc_len': 626, 'activation': 'sigmoid', 'units layer 1': 16, 'units layer 2': 16}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 5000, 'desc_len': 626, 'activation': 'sigmoid', 'units layer 1': 16, 'units layer 2': 16}\n",
      "2624/2624 - 10s - loss: 9.1868 - 10s/epoch - 4ms/step\n",
      "875/875 - 2s - loss: 9.2828 - 2s/epoch - 2ms/step\n",
      "Passing Parameters: {'desc_words': 5000, 'desc_len': 626, 'activation': 'sigmoid', 'units layer 1': 16, 'units layer 2': 32}\n",
      "Passing Parameters: {'desc_words': 5000, 'desc_len': 626, 'activation': 'sigmoid', 'units layer 1': 16, 'units layer 2': 64}\n",
      "Fitting Model 1, Parameters: {'desc_words': 5000, 'desc_len': 626, 'activation': 'sigmoid', 'units layer 1': 32, 'units layer 2': 4}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 5000, 'desc_len': 626, 'activation': 'sigmoid', 'units layer 1': 32, 'units layer 2': 4}\n",
      "2624/2624 - 9s - loss: 9.1757 - 9s/epoch - 4ms/step\n",
      "875/875 - 3s - loss: 9.2723 - 3s/epoch - 3ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 5000, 'desc_len': 626, 'activation': 'sigmoid', 'units layer 1': 32, 'units layer 2': 8}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 5000, 'desc_len': 626, 'activation': 'sigmoid', 'units layer 1': 32, 'units layer 2': 8}\n",
      "2624/2624 - 9s - loss: 9.1746 - 9s/epoch - 4ms/step\n",
      "875/875 - 4s - loss: 9.2709 - 4s/epoch - 4ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 5000, 'desc_len': 626, 'activation': 'sigmoid', 'units layer 1': 32, 'units layer 2': 16}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 5000, 'desc_len': 626, 'activation': 'sigmoid', 'units layer 1': 32, 'units layer 2': 16}\n",
      "2624/2624 - 11s - loss: 9.1734 - 11s/epoch - 4ms/step\n",
      "875/875 - 6s - loss: 9.2698 - 6s/epoch - 6ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 5000, 'desc_len': 626, 'activation': 'sigmoid', 'units layer 1': 32, 'units layer 2': 32}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 5000, 'desc_len': 626, 'activation': 'sigmoid', 'units layer 1': 32, 'units layer 2': 32}\n",
      "2624/2624 - 13s - loss: 2.0345 - 13s/epoch - 5ms/step\n",
      "875/875 - 3s - loss: 2.7004 - 3s/epoch - 4ms/step\n",
      "Passing Parameters: {'desc_words': 5000, 'desc_len': 626, 'activation': 'sigmoid', 'units layer 1': 32, 'units layer 2': 64}\n",
      "Fitting Model 1, Parameters: {'desc_words': 5000, 'desc_len': 626, 'activation': 'sigmoid', 'units layer 1': 64, 'units layer 2': 4}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 5000, 'desc_len': 626, 'activation': 'sigmoid', 'units layer 1': 64, 'units layer 2': 4}\n",
      "2624/2624 - 15s - loss: 9.2318 - 15s/epoch - 6ms/step\n",
      "875/875 - 5s - loss: 9.3291 - 5s/epoch - 6ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 5000, 'desc_len': 626, 'activation': 'sigmoid', 'units layer 1': 64, 'units layer 2': 8}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 5000, 'desc_len': 626, 'activation': 'sigmoid', 'units layer 1': 64, 'units layer 2': 8}\n",
      "2624/2624 - 15s - loss: 9.1746 - 15s/epoch - 6ms/step\n",
      "875/875 - 3s - loss: 9.2712 - 3s/epoch - 4ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 5000, 'desc_len': 626, 'activation': 'sigmoid', 'units layer 1': 64, 'units layer 2': 16}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 5000, 'desc_len': 626, 'activation': 'sigmoid', 'units layer 1': 64, 'units layer 2': 16}\n",
      "2624/2624 - 15s - loss: 9.1798 - 15s/epoch - 6ms/step\n",
      "875/875 - 4s - loss: 9.2764 - 4s/epoch - 4ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 5000, 'desc_len': 626, 'activation': 'sigmoid', 'units layer 1': 64, 'units layer 2': 32}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 5000, 'desc_len': 626, 'activation': 'sigmoid', 'units layer 1': 64, 'units layer 2': 32}\n",
      "2624/2624 - 18s - loss: 9.1733 - 18s/epoch - 7ms/step\n",
      "875/875 - 4s - loss: 9.2697 - 4s/epoch - 4ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 5000, 'desc_len': 626, 'activation': 'sigmoid', 'units layer 1': 64, 'units layer 2': 64}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 5000, 'desc_len': 626, 'activation': 'sigmoid', 'units layer 1': 64, 'units layer 2': 64}\n",
      "2624/2624 - 17s - loss: 9.1951 - 17s/epoch - 6ms/step\n",
      "875/875 - 3s - loss: 9.2921 - 3s/epoch - 4ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 5000, 'desc_len': 626, 'activation': 'sigmoid', 'units layer 1': 128, 'units layer 2': 4}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 5000, 'desc_len': 626, 'activation': 'sigmoid', 'units layer 1': 128, 'units layer 2': 4}\n",
      "2624/2624 - 15s - loss: 215.6293 - 15s/epoch - 6ms/step\n",
      "875/875 - 5s - loss: 215.7752 - 5s/epoch - 6ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 5000, 'desc_len': 626, 'activation': 'sigmoid', 'units layer 1': 128, 'units layer 2': 8}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 5000, 'desc_len': 626, 'activation': 'sigmoid', 'units layer 1': 128, 'units layer 2': 8}\n",
      "2624/2624 - 32s - loss: 9.1736 - 32s/epoch - 12ms/step\n",
      "875/875 - 8s - loss: 9.2701 - 8s/epoch - 9ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 5000, 'desc_len': 626, 'activation': 'sigmoid', 'units layer 1': 128, 'units layer 2': 16}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 5000, 'desc_len': 626, 'activation': 'sigmoid', 'units layer 1': 128, 'units layer 2': 16}\n",
      "2624/2624 - 30s - loss: 9.1733 - 30s/epoch - 12ms/step\n",
      "875/875 - 11s - loss: 9.2697 - 11s/epoch - 12ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 5000, 'desc_len': 626, 'activation': 'sigmoid', 'units layer 1': 128, 'units layer 2': 32}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 5000, 'desc_len': 626, 'activation': 'sigmoid', 'units layer 1': 128, 'units layer 2': 32}\n",
      "2624/2624 - 29s - loss: 2.1151 - 29s/epoch - 11ms/step\n",
      "875/875 - 9s - loss: 2.6909 - 9s/epoch - 11ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 5000, 'desc_len': 626, 'activation': 'sigmoid', 'units layer 1': 128, 'units layer 2': 64}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 5000, 'desc_len': 626, 'activation': 'sigmoid', 'units layer 1': 128, 'units layer 2': 64}\n",
      "2624/2624 - 28s - loss: 2.1077 - 28s/epoch - 11ms/step\n",
      "875/875 - 9s - loss: 2.6569 - 9s/epoch - 10ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 5000, 'desc_len': 60, 'activation': 'relu', 'units layer 1': 8, 'units layer 2': 4}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 5000, 'desc_len': 60, 'activation': 'relu', 'units layer 1': 8, 'units layer 2': 4}\n",
      "2624/2624 - 5s - loss: 437.0185 - 5s/epoch - 2ms/step\n",
      "875/875 - 4s - loss: 437.4287 - 4s/epoch - 4ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 5000, 'desc_len': 60, 'activation': 'relu', 'units layer 1': 8, 'units layer 2': 8}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 5000, 'desc_len': 60, 'activation': 'relu', 'units layer 1': 8, 'units layer 2': 8}\n",
      "2624/2624 - 7s - loss: 1355.4749 - 7s/epoch - 3ms/step\n",
      "875/875 - 3s - loss: 1356.5176 - 3s/epoch - 4ms/step\n",
      "Passing Parameters: {'desc_words': 5000, 'desc_len': 60, 'activation': 'relu', 'units layer 1': 8, 'units layer 2': 16}\n",
      "Passing Parameters: {'desc_words': 5000, 'desc_len': 60, 'activation': 'relu', 'units layer 1': 8, 'units layer 2': 32}\n",
      "Passing Parameters: {'desc_words': 5000, 'desc_len': 60, 'activation': 'relu', 'units layer 1': 8, 'units layer 2': 64}\n",
      "Fitting Model 1, Parameters: {'desc_words': 5000, 'desc_len': 60, 'activation': 'relu', 'units layer 1': 16, 'units layer 2': 4}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 5000, 'desc_len': 60, 'activation': 'relu', 'units layer 1': 16, 'units layer 2': 4}\n",
      "2624/2624 - 8s - loss: 1118.2495 - 8s/epoch - 3ms/step\n",
      "875/875 - 3s - loss: 1119.1200 - 3s/epoch - 3ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 5000, 'desc_len': 60, 'activation': 'relu', 'units layer 1': 16, 'units layer 2': 8}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 5000, 'desc_len': 60, 'activation': 'relu', 'units layer 1': 16, 'units layer 2': 8}\n",
      "2624/2624 - 6s - loss: 1349.9464 - 6s/epoch - 2ms/step\n",
      "875/875 - 2s - loss: 1350.7352 - 2s/epoch - 2ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 5000, 'desc_len': 60, 'activation': 'relu', 'units layer 1': 16, 'units layer 2': 16}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 5000, 'desc_len': 60, 'activation': 'relu', 'units layer 1': 16, 'units layer 2': 16}\n",
      "2624/2624 - 4s - loss: 1160.5500 - 4s/epoch - 2ms/step\n",
      "875/875 - 1s - loss: 1161.4960 - 1s/epoch - 2ms/step\n",
      "Passing Parameters: {'desc_words': 5000, 'desc_len': 60, 'activation': 'relu', 'units layer 1': 16, 'units layer 2': 32}\n",
      "Passing Parameters: {'desc_words': 5000, 'desc_len': 60, 'activation': 'relu', 'units layer 1': 16, 'units layer 2': 64}\n",
      "Fitting Model 1, Parameters: {'desc_words': 5000, 'desc_len': 60, 'activation': 'relu', 'units layer 1': 32, 'units layer 2': 4}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 5000, 'desc_len': 60, 'activation': 'relu', 'units layer 1': 32, 'units layer 2': 4}\n",
      "2624/2624 - 4s - loss: 1078.6925 - 4s/epoch - 2ms/step\n",
      "875/875 - 1s - loss: 1079.2310 - 1s/epoch - 2ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 5000, 'desc_len': 60, 'activation': 'relu', 'units layer 1': 32, 'units layer 2': 8}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 5000, 'desc_len': 60, 'activation': 'relu', 'units layer 1': 32, 'units layer 2': 8}\n",
      "2624/2624 - 6s - loss: 1021.3055 - 6s/epoch - 2ms/step\n",
      "875/875 - 2s - loss: 1021.9026 - 2s/epoch - 2ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 5000, 'desc_len': 60, 'activation': 'relu', 'units layer 1': 32, 'units layer 2': 16}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 5000, 'desc_len': 60, 'activation': 'relu', 'units layer 1': 32, 'units layer 2': 16}\n",
      "2624/2624 - 4s - loss: 1191.7039 - 4s/epoch - 2ms/step\n",
      "875/875 - 1s - loss: 1192.6691 - 1s/epoch - 2ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 5000, 'desc_len': 60, 'activation': 'relu', 'units layer 1': 32, 'units layer 2': 32}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 5000, 'desc_len': 60, 'activation': 'relu', 'units layer 1': 32, 'units layer 2': 32}\n",
      "2624/2624 - 4s - loss: 1318.1003 - 4s/epoch - 2ms/step\n",
      "875/875 - 2s - loss: 1319.4291 - 2s/epoch - 2ms/step\n",
      "Passing Parameters: {'desc_words': 5000, 'desc_len': 60, 'activation': 'relu', 'units layer 1': 32, 'units layer 2': 64}\n",
      "Fitting Model 1, Parameters: {'desc_words': 5000, 'desc_len': 60, 'activation': 'relu', 'units layer 1': 64, 'units layer 2': 4}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 5000, 'desc_len': 60, 'activation': 'relu', 'units layer 1': 64, 'units layer 2': 4}\n",
      "2624/2624 - 4s - loss: 2.1461 - 4s/epoch - 2ms/step\n",
      "875/875 - 1s - loss: 2.8059 - 1s/epoch - 2ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 5000, 'desc_len': 60, 'activation': 'relu', 'units layer 1': 64, 'units layer 2': 8}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 5000, 'desc_len': 60, 'activation': 'relu', 'units layer 1': 64, 'units layer 2': 8}\n",
      "2624/2624 - 5s - loss: 1011.3353 - 5s/epoch - 2ms/step\n",
      "875/875 - 2s - loss: 1011.8243 - 2s/epoch - 2ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 5000, 'desc_len': 60, 'activation': 'relu', 'units layer 1': 64, 'units layer 2': 16}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 5000, 'desc_len': 60, 'activation': 'relu', 'units layer 1': 64, 'units layer 2': 16}\n",
      "2624/2624 - 4s - loss: 980.0402 - 4s/epoch - 2ms/step\n",
      "875/875 - 2s - loss: 980.4605 - 2s/epoch - 2ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 5000, 'desc_len': 60, 'activation': 'relu', 'units layer 1': 64, 'units layer 2': 32}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 5000, 'desc_len': 60, 'activation': 'relu', 'units layer 1': 64, 'units layer 2': 32}\n",
      "2624/2624 - 4s - loss: 1267.9490 - 4s/epoch - 2ms/step\n",
      "875/875 - 3s - loss: 1268.8022 - 3s/epoch - 3ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 5000, 'desc_len': 60, 'activation': 'relu', 'units layer 1': 64, 'units layer 2': 64}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 5000, 'desc_len': 60, 'activation': 'relu', 'units layer 1': 64, 'units layer 2': 64}\n",
      "2624/2624 - 4s - loss: 1291.5145 - 4s/epoch - 2ms/step\n",
      "875/875 - 1s - loss: 1292.1090 - 1s/epoch - 2ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 5000, 'desc_len': 60, 'activation': 'relu', 'units layer 1': 128, 'units layer 2': 4}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 5000, 'desc_len': 60, 'activation': 'relu', 'units layer 1': 128, 'units layer 2': 4}\n",
      "2624/2624 - 5s - loss: 6.8021 - 5s/epoch - 2ms/step\n",
      "875/875 - 2s - loss: 7.1675 - 2s/epoch - 2ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 5000, 'desc_len': 60, 'activation': 'relu', 'units layer 1': 128, 'units layer 2': 8}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 5000, 'desc_len': 60, 'activation': 'relu', 'units layer 1': 128, 'units layer 2': 8}\n",
      "2624/2624 - 5s - loss: 1.9934 - 5s/epoch - 2ms/step\n",
      "875/875 - 2s - loss: 2.8290 - 2s/epoch - 2ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 5000, 'desc_len': 60, 'activation': 'relu', 'units layer 1': 128, 'units layer 2': 16}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 5000, 'desc_len': 60, 'activation': 'relu', 'units layer 1': 128, 'units layer 2': 16}\n",
      "2624/2624 - 5s - loss: 1.9098 - 5s/epoch - 2ms/step\n",
      "875/875 - 1s - loss: 2.8380 - 1s/epoch - 2ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 5000, 'desc_len': 60, 'activation': 'relu', 'units layer 1': 128, 'units layer 2': 32}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 5000, 'desc_len': 60, 'activation': 'relu', 'units layer 1': 128, 'units layer 2': 32}\n",
      "2624/2624 - 5s - loss: 732.6411 - 5s/epoch - 2ms/step\n",
      "875/875 - 2s - loss: 733.1082 - 2s/epoch - 2ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 5000, 'desc_len': 60, 'activation': 'relu', 'units layer 1': 128, 'units layer 2': 64}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 5000, 'desc_len': 60, 'activation': 'relu', 'units layer 1': 128, 'units layer 2': 64}\n",
      "2624/2624 - 5s - loss: 671.0881 - 5s/epoch - 2ms/step\n",
      "875/875 - 2s - loss: 671.2286 - 2s/epoch - 2ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 5000, 'desc_len': 60, 'activation': 'sigmoid', 'units layer 1': 8, 'units layer 2': 4}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 5000, 'desc_len': 60, 'activation': 'sigmoid', 'units layer 1': 8, 'units layer 2': 4}\n",
      "2624/2624 - 3s - loss: 9.1734 - 3s/epoch - 1ms/step\n",
      "875/875 - 1s - loss: 9.2698 - 1s/epoch - 1ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 5000, 'desc_len': 60, 'activation': 'sigmoid', 'units layer 1': 8, 'units layer 2': 8}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 5000, 'desc_len': 60, 'activation': 'sigmoid', 'units layer 1': 8, 'units layer 2': 8}\n",
      "2624/2624 - 3s - loss: 9.1736 - 3s/epoch - 1ms/step\n",
      "875/875 - 1s - loss: 9.2701 - 1s/epoch - 2ms/step\n",
      "Passing Parameters: {'desc_words': 5000, 'desc_len': 60, 'activation': 'sigmoid', 'units layer 1': 8, 'units layer 2': 16}\n",
      "Passing Parameters: {'desc_words': 5000, 'desc_len': 60, 'activation': 'sigmoid', 'units layer 1': 8, 'units layer 2': 32}\n",
      "Passing Parameters: {'desc_words': 5000, 'desc_len': 60, 'activation': 'sigmoid', 'units layer 1': 8, 'units layer 2': 64}\n",
      "Fitting Model 1, Parameters: {'desc_words': 5000, 'desc_len': 60, 'activation': 'sigmoid', 'units layer 1': 16, 'units layer 2': 4}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 5000, 'desc_len': 60, 'activation': 'sigmoid', 'units layer 1': 16, 'units layer 2': 4}\n",
      "2624/2624 - 3s - loss: 9.1754 - 3s/epoch - 1ms/step\n",
      "875/875 - 1s - loss: 9.2720 - 1s/epoch - 2ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 5000, 'desc_len': 60, 'activation': 'sigmoid', 'units layer 1': 16, 'units layer 2': 8}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 5000, 'desc_len': 60, 'activation': 'sigmoid', 'units layer 1': 16, 'units layer 2': 8}\n",
      "2624/2624 - 3s - loss: 5.7354 - 3s/epoch - 1ms/step\n",
      "875/875 - 1s - loss: 5.8086 - 1s/epoch - 1ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 5000, 'desc_len': 60, 'activation': 'sigmoid', 'units layer 1': 16, 'units layer 2': 16}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 5000, 'desc_len': 60, 'activation': 'sigmoid', 'units layer 1': 16, 'units layer 2': 16}\n",
      "2624/2624 - 4s - loss: 2.5652 - 4s/epoch - 2ms/step\n",
      "875/875 - 1s - loss: 2.9800 - 1s/epoch - 1ms/step\n",
      "Passing Parameters: {'desc_words': 5000, 'desc_len': 60, 'activation': 'sigmoid', 'units layer 1': 16, 'units layer 2': 32}\n",
      "Passing Parameters: {'desc_words': 5000, 'desc_len': 60, 'activation': 'sigmoid', 'units layer 1': 16, 'units layer 2': 64}\n",
      "Fitting Model 1, Parameters: {'desc_words': 5000, 'desc_len': 60, 'activation': 'sigmoid', 'units layer 1': 32, 'units layer 2': 4}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 5000, 'desc_len': 60, 'activation': 'sigmoid', 'units layer 1': 32, 'units layer 2': 4}\n",
      "2624/2624 - 6s - loss: 6.0004 - 6s/epoch - 2ms/step\n",
      "875/875 - 2s - loss: 6.0479 - 2s/epoch - 2ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 5000, 'desc_len': 60, 'activation': 'sigmoid', 'units layer 1': 32, 'units layer 2': 8}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 5000, 'desc_len': 60, 'activation': 'sigmoid', 'units layer 1': 32, 'units layer 2': 8}\n",
      "2624/2624 - 5s - loss: 9.1734 - 5s/epoch - 2ms/step\n",
      "875/875 - 2s - loss: 9.2698 - 2s/epoch - 2ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 5000, 'desc_len': 60, 'activation': 'sigmoid', 'units layer 1': 32, 'units layer 2': 16}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 5000, 'desc_len': 60, 'activation': 'sigmoid', 'units layer 1': 32, 'units layer 2': 16}\n",
      "2624/2624 - 4s - loss: 2.2827 - 4s/epoch - 2ms/step\n",
      "875/875 - 2s - loss: 2.6863 - 2s/epoch - 2ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 5000, 'desc_len': 60, 'activation': 'sigmoid', 'units layer 1': 32, 'units layer 2': 32}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 5000, 'desc_len': 60, 'activation': 'sigmoid', 'units layer 1': 32, 'units layer 2': 32}\n",
      "2624/2624 - 5s - loss: 9.1845 - 5s/epoch - 2ms/step\n",
      "875/875 - 1s - loss: 9.2813 - 1s/epoch - 2ms/step\n",
      "Passing Parameters: {'desc_words': 5000, 'desc_len': 60, 'activation': 'sigmoid', 'units layer 1': 32, 'units layer 2': 64}\n",
      "Fitting Model 1, Parameters: {'desc_words': 5000, 'desc_len': 60, 'activation': 'sigmoid', 'units layer 1': 64, 'units layer 2': 4}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 5000, 'desc_len': 60, 'activation': 'sigmoid', 'units layer 1': 64, 'units layer 2': 4}\n",
      "2624/2624 - 5s - loss: 32.2810 - 5s/epoch - 2ms/step\n",
      "875/875 - 2s - loss: 32.3940 - 2s/epoch - 2ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 5000, 'desc_len': 60, 'activation': 'sigmoid', 'units layer 1': 64, 'units layer 2': 8}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 5000, 'desc_len': 60, 'activation': 'sigmoid', 'units layer 1': 64, 'units layer 2': 8}\n",
      "2624/2624 - 6s - loss: 9.1741 - 6s/epoch - 2ms/step\n",
      "875/875 - 2s - loss: 9.2706 - 2s/epoch - 2ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 5000, 'desc_len': 60, 'activation': 'sigmoid', 'units layer 1': 64, 'units layer 2': 16}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 5000, 'desc_len': 60, 'activation': 'sigmoid', 'units layer 1': 64, 'units layer 2': 16}\n",
      "2624/2624 - 5s - loss: 2.1555 - 5s/epoch - 2ms/step\n",
      "875/875 - 2s - loss: 2.5922 - 2s/epoch - 2ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 5000, 'desc_len': 60, 'activation': 'sigmoid', 'units layer 1': 64, 'units layer 2': 32}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 5000, 'desc_len': 60, 'activation': 'sigmoid', 'units layer 1': 64, 'units layer 2': 32}\n",
      "2624/2624 - 6s - loss: 2.0507 - 6s/epoch - 2ms/step\n",
      "875/875 - 2s - loss: 2.6511 - 2s/epoch - 2ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 5000, 'desc_len': 60, 'activation': 'sigmoid', 'units layer 1': 64, 'units layer 2': 64}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 5000, 'desc_len': 60, 'activation': 'sigmoid', 'units layer 1': 64, 'units layer 2': 64}\n",
      "2624/2624 - 5s - loss: 9.1793 - 5s/epoch - 2ms/step\n",
      "875/875 - 2s - loss: 9.2755 - 2s/epoch - 2ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 5000, 'desc_len': 60, 'activation': 'sigmoid', 'units layer 1': 128, 'units layer 2': 4}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 5000, 'desc_len': 60, 'activation': 'sigmoid', 'units layer 1': 128, 'units layer 2': 4}\n",
      "2624/2624 - 5s - loss: 9.1846 - 5s/epoch - 2ms/step\n",
      "875/875 - 2s - loss: 9.2814 - 2s/epoch - 2ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 5000, 'desc_len': 60, 'activation': 'sigmoid', 'units layer 1': 128, 'units layer 2': 8}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 5000, 'desc_len': 60, 'activation': 'sigmoid', 'units layer 1': 128, 'units layer 2': 8}\n",
      "2624/2624 - 5s - loss: 9.1752 - 5s/epoch - 2ms/step\n",
      "875/875 - 2s - loss: 9.2718 - 2s/epoch - 2ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 5000, 'desc_len': 60, 'activation': 'sigmoid', 'units layer 1': 128, 'units layer 2': 16}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 5000, 'desc_len': 60, 'activation': 'sigmoid', 'units layer 1': 128, 'units layer 2': 16}\n",
      "2624/2624 - 5s - loss: 4.2936 - 5s/epoch - 2ms/step\n",
      "875/875 - 2s - loss: 4.3985 - 2s/epoch - 2ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 5000, 'desc_len': 60, 'activation': 'sigmoid', 'units layer 1': 128, 'units layer 2': 32}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 5000, 'desc_len': 60, 'activation': 'sigmoid', 'units layer 1': 128, 'units layer 2': 32}\n",
      "2624/2624 - 5s - loss: 9.1751 - 5s/epoch - 2ms/step\n",
      "875/875 - 2s - loss: 9.2714 - 2s/epoch - 2ms/step\n",
      "Fitting Model 1, Parameters: {'desc_words': 5000, 'desc_len': 60, 'activation': 'sigmoid', 'units layer 1': 128, 'units layer 2': 64}\n",
      "Evaluating Model 1, Parameters: {'desc_words': 5000, 'desc_len': 60, 'activation': 'sigmoid', 'units layer 1': 128, 'units layer 2': 64}\n",
      "2624/2624 - 5s - loss: 1.7949 - 5s/epoch - 2ms/step\n",
      "875/875 - 2s - loss: 2.6835 - 2s/epoch - 2ms/step\n"
     ]
    }
   ],
   "source": [
    "categorical_cols = ['country', 'province', 'region_1', 'variety', 'winery', 'year']\n",
    "numerical_cols = ['price']\n",
    "\n",
    "max_description_length = max(wine_reviews.description_length)\n",
    "max_description_length\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(wr_work[categorical_cols + numerical_cols], wr_work.points, \\\n",
    "                                                    test_size = 0.25, shuffle = True, random_state = 29)\n",
    "\n",
    "\n",
    "desc_words = [1000, 5000]\n",
    "activations = ['relu', 'sigmoid']\n",
    "layer_1_dim =  [4, 8, 16, 32, 64]\n",
    "layer_2_dim = [2, 4, 8, 16, 32, 64]\n",
    "model_1_results_df = pd.DataFrame(columns = ['parameters', 'train_MSE', 'test_MSE'])\n",
    "\n",
    "for desc in desc_words:\n",
    "    for d_len in desc_len:\n",
    "        for activation in activations:\n",
    "            for l1_dim in layer_1_dim:\n",
    "                for l2_dim in layer_2_dim:\n",
    "                    params = {'desc_words' : a, 'desc_len': b, 'activation': c,  'units layer 1': d, 'units layer 2': e} \n",
    "                    tokenizer = Tokenizer(num_words=desc_words)\n",
    "                    tokenizer.fit_on_texts(wr_work.description)\n",
    "                    desc = tokenizer_1000.texts_to_sequences(wine_reviews.description)\n",
    "                    desc_max = pad_sequences(desc_1000, maxlen=max_description_length)\n",
    "                    desc_60 = pad_sequences(desc_1000, maxlen=60)\n",
    "                    data_input = Input(shape=(desc_len,))\n",
    "                    embedding = Embedding(input_dim = desc, output_dim=16)(data_input)\n",
    "                    flatten = Flatten()(embedding)\n",
    "                    dense = Dense(units = l1_dim, activation = activation)(flatten)\n",
    "                    drop =  Dropout(0.5)(dense)\n",
    "                    dense = Dense(units = l2_dim, activation= activation)(drop)\n",
    "                    output = Dense(units = 1, activation= 'linear')(dense)\n",
    "                    model = Model(inputs=[data_input], outputs=output)\n",
    "\n",
    "                    model.compile(optimizer='adam', loss='r2')\n",
    "                        \n",
    "                    print(f'Fitting Model 1, Parameters: {params}')\n",
    "                    model.fit(x_train, y_train,\n",
    "                                batch_size=32,\n",
    "                                epochs=10,\n",
    "                                callbacks=EarlyStopping(monitor='val_loss', patience=2),\n",
    "                                workers = 8,\n",
    "                                verbose = 0,\n",
    "                        validation_data=(x_test, y_test))\n",
    "                    print(f'Evaluating Model 1, Parameters: {params}')\n",
    "                    train_MSE = model_1.evaluate(x_train, y_train, verbose = 42)\n",
    "                    test_MSE = model_1.evaluate(x_test, y_test, verbose = 42)\n",
    "            \n",
    "                    summary_df.loc[len(summary_df.index)] = (['Embedding', params, train_MSE, test_MSE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 files belonging to 1 classes.\n",
      "Using 0 files for training.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No text files found in directory ./. Allowed format: .txt",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [77]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m text_dataset_from_directory\n\u001b[1;32m----> 2\u001b[0m raw_train_ds \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext_dataset_from_directory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtraining\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m29\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m class_names \u001b[38;5;241m=\u001b[39m raw_train_ds\u001b[38;5;241m.\u001b[39mclass_names\n\u001b[0;32m     10\u001b[0m train_ds \u001b[38;5;241m=\u001b[39m raw_train_ds\u001b[38;5;241m.\u001b[39mcache()\u001b[38;5;241m.\u001b[39mprefetch(buffer_size\u001b[38;5;241m=\u001b[39mAUTOTUNE)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\text_dataset.py:236\u001b[0m, in \u001b[0;36mtext_dataset_from_directory\u001b[1;34m(directory, labels, label_mode, class_names, batch_size, max_length, shuffle, seed, validation_split, subset, follow_links)\u001b[0m\n\u001b[0;32m    232\u001b[0m file_paths, labels \u001b[38;5;241m=\u001b[39m dataset_utils\u001b[38;5;241m.\u001b[39mget_training_or_validation_split(\n\u001b[0;32m    233\u001b[0m     file_paths, labels, validation_split, subset\n\u001b[0;32m    234\u001b[0m )\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m file_paths:\n\u001b[1;32m--> 236\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    237\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo text files found in directory \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdirectory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    238\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAllowed format: .txt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    239\u001b[0m     )\n\u001b[0;32m    240\u001b[0m dataset \u001b[38;5;241m=\u001b[39m paths_and_labels_to_dataset(\n\u001b[0;32m    241\u001b[0m     file_paths\u001b[38;5;241m=\u001b[39mfile_paths,\n\u001b[0;32m    242\u001b[0m     labels\u001b[38;5;241m=\u001b[39mlabels,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    245\u001b[0m     max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[0;32m    246\u001b[0m )\n\u001b[0;32m    247\u001b[0m dataset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mprefetch(tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mAUTOTUNE)\n",
      "\u001b[1;31mValueError\u001b[0m: No text files found in directory ./. Allowed format: .txt"
     ]
    }
   ],
   "source": [
    "model_1_results_df.sort_values(by='train_MSE',ascending=True)\n",
    "\n",
    "wr_work = wine_reviews[['description','country','price','province','region_1','variety','winery','year','points']]\n",
    "wr_work.head()\n",
    "\n",
    "categorical_cols = ['country', 'province', 'region_1', 'variety', 'winery', 'year']\n",
    "numerical_cols = ['price']\n",
    "\n",
    "max_desc_len = max(wine_reviews.desc_len)\n",
    "max_desc_len\n",
    "\n",
    "tokenizer_1000 = Tokenizer(num_words=1000)\n",
    "tokenizer_1000.fit_on_texts(wr_work.description)\n",
    "desc_1000 = tokenizer_1000.texts_to_sequences(wr_work.description)\n",
    "desc_1000_max = pad_sequences(desc_1000, maxlen=max_desc_len)\n",
    "desc_1000_60 = pad_sequences(desc_1000, maxlen=60)\n",
    "desc_1000_60\n",
    "\n",
    "tokenizer_5000 = Tokenizer(num_words=5000)\n",
    "tokenizer_5000.fit_on_texts(wr_work.description)\n",
    "desc_5000 = tokenizer_5000.texts_to_sequences(wr_work.description)\n",
    "\n",
    "desc_5000_max = pad_sequences(desc_5000, maxlen=max_desc_len)\n",
    "desc_5000_60 = pad_sequences(desc_5000, maxlen=60)\n",
    "desc_5000_60\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(wr_work[categorical_cols + numerical_cols], wr_work.points, \\\n",
    "                                                    test_size = 0.25, shuffle = True, random_state = 78)\n",
    "\n",
    "\n",
    "desc_1000_max_train, desc_1000_max_test = train_test_split(desc_1000_max, test_size = 0.25, shuffle = True, random_state = 78)\n",
    "desc_1000_60_train, desc_1000_60_test = train_test_split(desc_1000_60, test_size = 0.25, shuffle = True, random_state = 78)\n",
    "desc_5000_max_train, desc_5000_max_test = train_test_split(desc_5000_max, test_size = 0.25, shuffle = True, random_state = 78)\n",
    "desc_5000_60_train, desc_5000_60_test = train_test_split(desc_5000_60, test_size = 0.25, shuffle = True, random_state = 78)\n",
    "\n",
    "desc_words = [1000, 5000]\n",
    "desc_len = [max_desc_len, 60]\n",
    "dense_activations = ['relu', 'sigmoid']\n",
    "dense_units_1 =  [8, 16, 32, 64, 128]\n",
    "dense_units_2 = [4, 8, 16, 32, 64]\n",
    "model_1_results_df = pd.DataFrame(columns = ['parameters', 'train_MSE', 'test_MSE'])\n",
    "\n",
    "for a in desc_words:\n",
    "    for b in desc_len:\n",
    "        for c in dense_activations:\n",
    "            for d in dense_units_1:\n",
    "                for e in dense_units_2:\n",
    "                    params = {'desc_words' : a, 'desc_len': b, 'activation': c,  'units layer 1': d, 'units layer 2': e}\n",
    "                    if e > d:\n",
    "                        print(f'Passing Parameters: {params}')\n",
    "                        continue  \n",
    "                    input_1 = Input(shape=(b,))\n",
    "                    embedding_1 = Embedding(input_dim = a, output_dim=10)(input_1)\n",
    "                    flatten_1 = Flatten()(embedding_1)\n",
    "                    dense_1a = Dense(units = d, activation = c)(flatten_1)\n",
    "                    drop_1 =  Dropout(0.5)(dense_1a)\n",
    "                    dense_1b = Dense(units = e, activation= c)(drop_1)\n",
    "                    output_1 = Dense(units = 1, activation= 'linear')(dense_1b)\n",
    "                    model_1 = Model(inputs=[input_1], outputs=output_1)\n",
    "\n",
    "                    model_1.compile(optimizer='adam', loss='mean_squared_error')\n",
    "                    \n",
    "                    if a == 1000 and b == max_desc_len:\n",
    "                        x_train_1, x_test_1 = desc_1000_max_train, desc_1000_max_test\n",
    "                    elif a == 1000 and b == 60:\n",
    "                        x_train_1, x_test_1 = desc_1000_60_train, desc_1000_60_test\n",
    "                    elif a == 5000 and b == max_desc_len:\n",
    "                        x_train_1, x_test_1 = desc_5000_max_train, desc_5000_max_test\n",
    "                    elif a == 5000 and b == 60:\n",
    "                        x_train_1, x_test_1 = desc_5000_60_train, desc_5000_60_test\n",
    "                        \n",
    "                    print(f'Fitting Model 1, Parameters: {params}')\n",
    "                    model_1.fit(x_train_1, y_train,\n",
    "                                batch_size=32,\n",
    "                                epochs=10,\n",
    "                                callbacks=EarlyStopping(monitor='val_loss', patience=3),\n",
    "                                workers = 8,\n",
    "                                verbose = 0,\n",
    "                        validation_data=(x_test_1, y_test))\n",
    "                    print(f'Evaluating Model 1, Parameters: {params}')\n",
    "                    train_MSE = model_1.evaluate(x_train_1, y_train, verbose = 2)\n",
    "                    test_MSE = model_1.evaluate(x_test_1, y_test, verbose = 2)\n",
    "            \n",
    "                    model_1_results_df.loc[len(model_1_results_df.index)] = ([params, train_MSE, test_MSE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'TFBertModel' object has no attribute 'encode'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [93]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m BertTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m model \u001b[38;5;241m=\u001b[39m TFBertModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m encoded_df \u001b[38;5;241m=\u001b[39m \u001b[43mdesc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapplymap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m output \u001b[38;5;241m=\u001b[39m model(encoded_input)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:8924\u001b[0m, in \u001b[0;36mDataFrame.applymap\u001b[1;34m(self, func, na_action, **kwargs)\u001b[0m\n\u001b[0;32m   8921\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer(x, func, ignore_na\u001b[38;5;241m=\u001b[39mignore_na)\n\u001b[0;32m   8922\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer(x\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values, func, ignore_na\u001b[38;5;241m=\u001b[39mignore_na)\n\u001b[1;32m-> 8924\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43minfer\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapplymap\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:8839\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[1;34m(self, func, axis, raw, result_type, args, **kwargs)\u001b[0m\n\u001b[0;32m   8828\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[0;32m   8830\u001b[0m op \u001b[38;5;241m=\u001b[39m frame_apply(\n\u001b[0;32m   8831\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   8832\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   8837\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[0;32m   8838\u001b[0m )\n\u001b[1;32m-> 8839\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py:727\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw:\n\u001b[0;32m    725\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_raw()\n\u001b[1;32m--> 727\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py:851\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    850\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 851\u001b[0m     results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_series_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    853\u001b[0m     \u001b[38;5;66;03m# wrap results\u001b[39;00m\n\u001b[0;32m    854\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrap_results(results, res_index)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py:867\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    864\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    865\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[0;32m    866\u001b[0m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[1;32m--> 867\u001b[0m         results[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    868\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[0;32m    869\u001b[0m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[0;32m    870\u001b[0m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[0;32m    871\u001b[0m             results[i] \u001b[38;5;241m=\u001b[39m results[i]\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:8922\u001b[0m, in \u001b[0;36mDataFrame.applymap.<locals>.infer\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m   8920\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mempty:\n\u001b[0;32m   8921\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer(x, func, ignore_na\u001b[38;5;241m=\u001b[39mignore_na)\n\u001b[1;32m-> 8922\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_na\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_na\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\lib.pyx:2870\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Input \u001b[1;32mIn [93]\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      5\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m BertTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m model \u001b[38;5;241m=\u001b[39m TFBertModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m encoded_df \u001b[38;5;241m=\u001b[39m desc\u001b[38;5;241m.\u001b[39mapplymap(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m(x, convert_to_tensor\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[0;32m      8\u001b[0m output \u001b[38;5;241m=\u001b[39m model(encoded_input)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'TFBertModel' object has no attribute 'encode'"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentencesDataset\n",
    "desc = pd.DataFrame(wine_reviews, columns=['description'])\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "encoded_df = desc.applymap(lambda x: model.encode(x, convert_to_tensor=True))\n",
    "output = model(encoded_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

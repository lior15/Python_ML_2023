{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wines Points prediction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submission Date : 3.6.2023\n",
    "Task: Predict the wine score given the inputs\n",
    "Instructions:\n",
    " * Use logistic regression as benchmark model\n",
    " * Use sklearn pipeliens + cv + grid search with sklearn models (e.g. KNNs, RandomForest, etc.)\n",
    " * Compare all models on proper metric (your choice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For DNN course project:\n",
    "* Use sklearn pipeliens with tensorflow models (w/wo embeddings, LSTMs, RNNs, Transformers etc.)\n",
    "* Compare all models on proper metric (your choice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys; sys.path.append('../')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will try to predict the points a wine will get based on known characteristics (i.e. features, in the ML terminology). The mine point in this stage is to establish a simple, ideally super cost effective, basline.\n",
    "In the real world there is a tradeoff between complexity and perforamnce, and the DS job, among others, is to present a tradeoff tables of what performance is achivalbel at what complexity level. \n",
    "\n",
    "to which models with increased complexity and resource demands will be compared. Complexity should then be translated into cost. For example:\n",
    " * Compute cost \n",
    " * Maintenance cost\n",
    " * Serving costs (i.e. is new platform needed?) \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.12.1.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import cufflinks as cf; cf.go_offline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding, Concatenate, Flatten, TextVectorization, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import MeanSquaredError\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical, plot_model, pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import activations\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lior\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:868: FutureWarning:\n",
      "\n",
      "`sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1012/1012 [==============================] - 80s 74ms/step - loss: 467.8390 - mse: 467.8390 - val_loss: 6.1965 - val_mse: 6.1965\n",
      "Epoch 2/10\n",
      "1012/1012 [==============================] - 137s 135ms/step - loss: 5.9820 - mse: 5.9820 - val_loss: 5.8643 - val_mse: 5.8643\n",
      "Epoch 3/10\n",
      "1012/1012 [==============================] - 81s 80ms/step - loss: 5.8069 - mse: 5.8069 - val_loss: 5.8129 - val_mse: 5.8129\n",
      "Epoch 4/10\n",
      "1012/1012 [==============================] - 73s 72ms/step - loss: 5.7820 - mse: 5.7820 - val_loss: 5.7795 - val_mse: 5.7795\n",
      "Epoch 5/10\n",
      "1012/1012 [==============================] - 93s 92ms/step - loss: 5.7959 - mse: 5.7959 - val_loss: 5.7881 - val_mse: 5.7881\n",
      "Epoch 6/10\n",
      "1012/1012 [==============================] - 151s 149ms/step - loss: 5.8024 - mse: 5.8024 - val_loss: 6.0474 - val_mse: 6.0474\n",
      "Epoch 7/10\n",
      "1012/1012 [==============================] - 129s 128ms/step - loss: 5.8364 - mse: 5.8364 - val_loss: 6.2460 - val_mse: 6.2460\n",
      "Epoch 8/10\n",
      "1012/1012 [==============================] - 101s 100ms/step - loss: 5.8259 - mse: 5.8259 - val_loss: 5.8068 - val_mse: 5.8068\n",
      "Epoch 9/10\n",
      "1012/1012 [==============================] - 101s 99ms/step - loss: 5.8324 - mse: 5.8324 - val_loss: 5.9040 - val_mse: 5.9040\n",
      "Epoch 10/10\n",
      "1012/1012 [==============================] - 145s 143ms/step - loss: 5.8482 - mse: 5.8482 - val_loss: 5.8347 - val_mse: 5.8347\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1fda4871850>"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Concatenate, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Load your dataset\n",
    "wine_reviews = pd.read_csv(\"clean_wine_reviews_data.csv\")\n",
    "summary_df = pd.read_csv(\"summary_df.csv\")\n",
    "\n",
    "# Preprocessing\n",
    "wine_reviews = wine_reviews.drop(['Unnamed: 0', 'taster_twitter_handle', 'region_1', 'region_2', 'designation'], axis=1)\n",
    "wine_reviews['description_length'] = wine_reviews.description.str.len()\n",
    "wine_reviews = wine_reviews.dropna(subset=['year', 'points'])\n",
    "wine_reviews['year'] = wine_reviews['year'].astype(int).astype(str)\n",
    "wine_reviews[\"description_length\"] = wine_reviews[\"description_length\"].astype(np.int32)\n",
    "\n",
    "# Split data\n",
    "y = wine_reviews['points']\n",
    "X = wine_reviews.drop(['points'], axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=29)\n",
    "\n",
    "# Preprocess categorical data\n",
    "categorical_cols = ['country', 'taster_name', 'variety', 'province', 'year']\n",
    "numeric_cols = ['price']\n",
    "\n",
    "# Use StandardScaler for numeric columns\n",
    "numeric_transformer = StandardScaler()\n",
    "\n",
    "# Use OneHotEncoder for categorical columns\n",
    "categorical_transformer = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    "\n",
    "# Preprocess features using ColumnTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ])\n",
    "\n",
    "# Fit and transform preprocessing on training data\n",
    "X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "X_test_transformed = preprocessor.transform(X_test)\n",
    "\n",
    "# Text Vectorization\n",
    "max_vocab_length = 10000\n",
    "max_length = 60\n",
    "\n",
    "text_vectorizer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=max_vocab_length,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=max_length)\n",
    "\n",
    "text_vectorizer.adapt(X_train['description'])\n",
    "text_input = Input(shape=(1,), dtype=\"string\")\n",
    "text_vector = text_vectorizer(text_input)\n",
    "text_embedding = Embedding(\n",
    "    input_dim=max_vocab_length,\n",
    "    output_dim=128,\n",
    "    embeddings_initializer=\"uniform\",\n",
    "    input_length=max_length)(text_vector)\n",
    "text_lstm = LSTM(64)(text_embedding)\n",
    "\n",
    "# Combine numeric and categorical data\n",
    "combined_input = Input(shape=(X_train_transformed.shape[1],), dtype=\"float32\")\n",
    "concatenated = Concatenate()([text_lstm, combined_input])\n",
    "\n",
    "# Build the model\n",
    "x = Dense(16, activation='relu')(concatenated)\n",
    "x = Dense(8, activation='relu')(x)\n",
    "output = Dense(1, activation=\"linear\")(x)\n",
    "\n",
    "model = Model(inputs=[text_input, combined_input], outputs=output)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss=tf.keras.losses.MeanSquaredError(),\n",
    "              optimizer=tf.keras.optimizers.Adam(),\n",
    "              metrics=['mse'])\n",
    "\n",
    "# Train the model\n",
    "model.fit([X_train['description'], X_train_transformed], y_train,\n",
    "          batch_size=64,\n",
    "          epochs=10,\n",
    "          validation_split=0.2) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with 16 layers, 8 units per layer, and learning rate 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lior\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:868: FutureWarning:\n",
      "\n",
      "`sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1012/1012 [==============================] - 73s 68ms/step - loss: 744.4686 - mse: 744.4686 - val_loss: 6.2972 - val_mse: 6.2972\n",
      "Epoch 2/10\n",
      "1012/1012 [==============================] - 70s 69ms/step - loss: 6.0590 - mse: 6.0590 - val_loss: 5.9055 - val_mse: 5.9055\n",
      "Epoch 3/10\n",
      "1012/1012 [==============================] - 69s 68ms/step - loss: 5.8431 - mse: 5.8431 - val_loss: 5.8243 - val_mse: 5.8243\n",
      "Epoch 4/10\n",
      "1012/1012 [==============================] - 69s 68ms/step - loss: 5.7957 - mse: 5.7957 - val_loss: 5.8830 - val_mse: 5.8830\n",
      "Epoch 5/10\n",
      "1012/1012 [==============================] - 69s 68ms/step - loss: 5.7864 - mse: 5.7864 - val_loss: 5.7897 - val_mse: 5.7897\n",
      "Epoch 6/10\n",
      "1012/1012 [==============================] - 69s 68ms/step - loss: 5.7934 - mse: 5.7934 - val_loss: 5.7667 - val_mse: 5.7667\n",
      "Epoch 7/10\n",
      "1012/1012 [==============================] - 69s 68ms/step - loss: 5.8043 - mse: 5.8043 - val_loss: 5.8434 - val_mse: 5.8434\n",
      "Epoch 8/10\n",
      "1012/1012 [==============================] - 68s 68ms/step - loss: 5.8112 - mse: 5.8112 - val_loss: 5.7907 - val_mse: 5.7907\n",
      "Epoch 9/10\n",
      "1012/1012 [==============================] - 69s 69ms/step - loss: 5.8102 - mse: 5.8102 - val_loss: 5.7705 - val_mse: 5.7705\n",
      "Epoch 10/10\n",
      "1012/1012 [==============================] - 69s 68ms/step - loss: 5.8122 - mse: 5.8122 - val_loss: 5.7857 - val_mse: 5.7857\n",
      "Training model with 16 layers, 8 units per layer, and learning rate 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lior\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:868: FutureWarning:\n",
      "\n",
      "`sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1012/1012 [==============================] - 71s 67ms/step - loss: 747.8009 - mse: 747.8009 - val_loss: 6.2652 - val_mse: 6.2652\n",
      "Epoch 2/10\n",
      "1012/1012 [==============================] - 68s 67ms/step - loss: 6.0320 - mse: 6.0320 - val_loss: 5.8923 - val_mse: 5.8923\n",
      "Epoch 3/10\n",
      "1012/1012 [==============================] - 67s 66ms/step - loss: 5.8326 - mse: 5.8326 - val_loss: 5.8514 - val_mse: 5.8514\n",
      "Epoch 4/10\n",
      "1012/1012 [==============================] - 68s 67ms/step - loss: 5.7841 - mse: 5.7841 - val_loss: 5.7778 - val_mse: 5.7778\n",
      "Epoch 5/10\n",
      "1012/1012 [==============================] - 67s 66ms/step - loss: 5.7798 - mse: 5.7798 - val_loss: 5.8106 - val_mse: 5.8106\n",
      "Epoch 6/10\n",
      "1012/1012 [==============================] - 68s 67ms/step - loss: 5.7873 - mse: 5.7873 - val_loss: 5.7989 - val_mse: 5.7989\n",
      "Epoch 7/10\n",
      "1012/1012 [==============================] - 67s 66ms/step - loss: 5.8161 - mse: 5.8161 - val_loss: 5.7759 - val_mse: 5.7759\n",
      "Epoch 8/10\n",
      "1012/1012 [==============================] - 67s 66ms/step - loss: 5.8053 - mse: 5.8053 - val_loss: 5.7733 - val_mse: 5.7733\n",
      "Epoch 9/10\n",
      "1012/1012 [==============================] - 67s 66ms/step - loss: 5.8061 - mse: 5.8061 - val_loss: 5.9481 - val_mse: 5.9481\n",
      "Epoch 10/10\n",
      "1012/1012 [==============================] - 67s 67ms/step - loss: 5.8171 - mse: 5.8171 - val_loss: 5.7752 - val_mse: 5.7752\n",
      "Training model with 16 layers, 164 units per layer, and learning rate 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lior\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:868: FutureWarning:\n",
      "\n",
      "`sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1012/1012 [==============================] - 79s 75ms/step - loss: 314.7740 - mse: 314.7740 - val_loss: 5.8437 - val_mse: 5.8437\n",
      "Epoch 2/10\n",
      "1012/1012 [==============================] - 71s 70ms/step - loss: 5.8455 - mse: 5.8455 - val_loss: 5.8710 - val_mse: 5.8710\n",
      "Epoch 3/10\n",
      "1012/1012 [==============================] - 71s 70ms/step - loss: 5.8370 - mse: 5.8370 - val_loss: 5.7931 - val_mse: 5.7931\n",
      "Epoch 4/10\n",
      "1012/1012 [==============================] - 71s 70ms/step - loss: 5.9081 - mse: 5.9081 - val_loss: 5.8534 - val_mse: 5.8534\n",
      "Epoch 5/10\n",
      "1012/1012 [==============================] - 71s 70ms/step - loss: 5.9992 - mse: 5.9992 - val_loss: 5.8594 - val_mse: 5.8594\n",
      "Epoch 6/10\n",
      "1012/1012 [==============================] - 72s 71ms/step - loss: 6.0126 - mse: 6.0126 - val_loss: 6.1214 - val_mse: 6.1214\n",
      "Epoch 7/10\n",
      "1012/1012 [==============================] - 71s 70ms/step - loss: 6.0021 - mse: 6.0021 - val_loss: 6.3186 - val_mse: 6.3186\n",
      "Epoch 8/10\n",
      "1012/1012 [==============================] - 71s 71ms/step - loss: 5.9647 - mse: 5.9647 - val_loss: 7.4636 - val_mse: 7.4636\n",
      "Epoch 9/10\n",
      "1012/1012 [==============================] - 72s 71ms/step - loss: 5.9662 - mse: 5.9662 - val_loss: 5.8766 - val_mse: 5.8766\n",
      "Epoch 10/10\n",
      "1012/1012 [==============================] - 73s 72ms/step - loss: 6.0121 - mse: 6.0121 - val_loss: 6.0666 - val_mse: 6.0666\n",
      "Training model with 16 layers, 164 units per layer, and learning rate 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lior\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:868: FutureWarning:\n",
      "\n",
      "`sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1012/1012 [==============================] - 78s 73ms/step - loss: 328.9190 - mse: 328.9190 - val_loss: 5.9344 - val_mse: 5.9344\n",
      "Epoch 2/10\n",
      "1012/1012 [==============================] - 79s 78ms/step - loss: 5.8347 - mse: 5.8347 - val_loss: 5.7947 - val_mse: 5.7947\n",
      "Epoch 3/10\n",
      "1012/1012 [==============================] - 78s 78ms/step - loss: 5.8554 - mse: 5.8554 - val_loss: 6.0965 - val_mse: 6.0965\n",
      "Epoch 4/10\n",
      "1012/1012 [==============================] - 79s 78ms/step - loss: 5.8888 - mse: 5.8888 - val_loss: 5.9578 - val_mse: 5.9578\n",
      "Epoch 5/10\n",
      "1012/1012 [==============================] - 79s 78ms/step - loss: 5.9460 - mse: 5.9460 - val_loss: 6.3907 - val_mse: 6.3907\n",
      "Epoch 6/10\n",
      "1012/1012 [==============================] - 78s 77ms/step - loss: 5.9819 - mse: 5.9819 - val_loss: 5.9202 - val_mse: 5.9202\n",
      "Epoch 7/10\n",
      "1012/1012 [==============================] - 78s 77ms/step - loss: 6.0462 - mse: 6.0462 - val_loss: 6.4108 - val_mse: 6.4108\n",
      "Epoch 8/10\n",
      "1012/1012 [==============================] - 78s 77ms/step - loss: 5.9891 - mse: 5.9891 - val_loss: 5.8603 - val_mse: 5.8603\n",
      "Epoch 9/10\n",
      "1012/1012 [==============================] - 78s 77ms/step - loss: 6.0092 - mse: 6.0092 - val_loss: 5.9233 - val_mse: 5.9233\n",
      "Epoch 10/10\n",
      "1012/1012 [==============================] - 79s 78ms/step - loss: 6.0041 - mse: 6.0041 - val_loss: 6.0975 - val_mse: 6.0975\n",
      "Training model with 32 layers, 8 units per layer, and learning rate 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lior\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:868: FutureWarning:\n",
      "\n",
      "`sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1012/1012 [==============================] - 79s 74ms/step - loss: 407.3505 - mse: 407.3505 - val_loss: 6.0203 - val_mse: 6.0203\n",
      "Epoch 2/10\n",
      "1012/1012 [==============================] - 74s 73ms/step - loss: 5.8756 - mse: 5.8756 - val_loss: 5.9657 - val_mse: 5.9657\n",
      "Epoch 3/10\n",
      "1012/1012 [==============================] - 76s 75ms/step - loss: 5.7941 - mse: 5.7941 - val_loss: 5.7885 - val_mse: 5.7885\n",
      "Epoch 4/10\n",
      "1012/1012 [==============================] - 74s 74ms/step - loss: 5.7921 - mse: 5.7921 - val_loss: 5.7787 - val_mse: 5.7787\n",
      "Epoch 5/10\n",
      "1012/1012 [==============================] - 74s 73ms/step - loss: 5.7947 - mse: 5.7947 - val_loss: 5.7882 - val_mse: 5.7882\n",
      "Epoch 6/10\n",
      "1012/1012 [==============================] - 75s 74ms/step - loss: 5.8581 - mse: 5.8581 - val_loss: 6.1370 - val_mse: 6.1370\n",
      "Epoch 7/10\n",
      "1012/1012 [==============================] - 74s 73ms/step - loss: 5.8614 - mse: 5.8614 - val_loss: 5.9252 - val_mse: 5.9252\n",
      "Epoch 8/10\n",
      "1012/1012 [==============================] - 74s 74ms/step - loss: 5.8514 - mse: 5.8514 - val_loss: 5.8020 - val_mse: 5.8020\n",
      "Epoch 9/10\n",
      "1012/1012 [==============================] - 74s 73ms/step - loss: 5.8815 - mse: 5.8815 - val_loss: 5.8462 - val_mse: 5.8462\n",
      "Epoch 10/10\n",
      "1012/1012 [==============================] - 74s 73ms/step - loss: 5.8555 - mse: 5.8555 - val_loss: 5.8622 - val_mse: 5.8622\n",
      "Training model with 32 layers, 8 units per layer, and learning rate 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lior\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:868: FutureWarning:\n",
      "\n",
      "`sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1012/1012 [==============================] - 83s 78ms/step - loss: 508.4413 - mse: 508.4413 - val_loss: 6.1886 - val_mse: 6.1886\n",
      "Epoch 2/10\n",
      "1012/1012 [==============================] - 77s 76ms/step - loss: 5.9791 - mse: 5.9791 - val_loss: 5.8602 - val_mse: 5.8602\n",
      "Epoch 3/10\n",
      "1012/1012 [==============================] - 76s 75ms/step - loss: 5.8150 - mse: 5.8150 - val_loss: 5.8121 - val_mse: 5.8121\n",
      "Epoch 4/10\n",
      "1012/1012 [==============================] - 85s 84ms/step - loss: 5.7813 - mse: 5.7813 - val_loss: 5.7835 - val_mse: 5.7835\n",
      "Epoch 5/10\n",
      "1012/1012 [==============================] - 78s 77ms/step - loss: 5.7904 - mse: 5.7904 - val_loss: 5.7909 - val_mse: 5.7909\n",
      "Epoch 6/10\n",
      "1012/1012 [==============================] - 78s 77ms/step - loss: 5.8002 - mse: 5.8002 - val_loss: 5.7933 - val_mse: 5.7933\n",
      "Epoch 7/10\n",
      "1012/1012 [==============================] - 79s 78ms/step - loss: 5.8214 - mse: 5.8214 - val_loss: 5.7758 - val_mse: 5.7758\n",
      "Epoch 8/10\n",
      "1012/1012 [==============================] - 80s 79ms/step - loss: 5.8394 - mse: 5.8394 - val_loss: 5.7851 - val_mse: 5.7851\n",
      "Epoch 9/10\n",
      "1012/1012 [==============================] - 79s 78ms/step - loss: 5.8516 - mse: 5.8516 - val_loss: 5.8418 - val_mse: 5.8418\n",
      "Epoch 10/10\n",
      "1012/1012 [==============================] - 80s 79ms/step - loss: 5.8226 - mse: 5.8226 - val_loss: 6.0759 - val_mse: 6.0759\n",
      "Training model with 32 layers, 164 units per layer, and learning rate 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lior\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:868: FutureWarning:\n",
      "\n",
      "`sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1012/1012 [==============================] - 80s 75ms/step - loss: 273.4088 - mse: 273.4088 - val_loss: 5.8429 - val_mse: 5.8429\n",
      "Epoch 2/10\n",
      "1012/1012 [==============================] - 76s 75ms/step - loss: 5.8388 - mse: 5.8388 - val_loss: 5.9686 - val_mse: 5.9686\n",
      "Epoch 3/10\n",
      "1012/1012 [==============================] - 76s 75ms/step - loss: 5.8713 - mse: 5.8713 - val_loss: 5.8381 - val_mse: 5.8381\n",
      "Epoch 4/10\n",
      "1012/1012 [==============================] - 77s 76ms/step - loss: 5.9346 - mse: 5.9346 - val_loss: 5.8894 - val_mse: 5.8894\n",
      "Epoch 5/10\n",
      "1012/1012 [==============================] - 76s 75ms/step - loss: 6.0305 - mse: 6.0305 - val_loss: 7.7384 - val_mse: 7.7384\n",
      "Epoch 6/10\n",
      "1012/1012 [==============================] - 77s 76ms/step - loss: 6.0413 - mse: 6.0413 - val_loss: 6.7371 - val_mse: 6.7371\n",
      "Epoch 7/10\n",
      "1012/1012 [==============================] - 77s 77ms/step - loss: 6.0892 - mse: 6.0892 - val_loss: 6.1751 - val_mse: 6.1751\n",
      "Epoch 8/10\n",
      "1012/1012 [==============================] - 78s 77ms/step - loss: 6.0614 - mse: 6.0614 - val_loss: 6.0040 - val_mse: 6.0040\n",
      "Epoch 9/10\n",
      "1012/1012 [==============================] - 77s 76ms/step - loss: 6.0543 - mse: 6.0543 - val_loss: 5.8407 - val_mse: 5.8407\n",
      "Epoch 10/10\n",
      "1012/1012 [==============================] - 79s 78ms/step - loss: 5.9820 - mse: 5.9820 - val_loss: 6.7461 - val_mse: 6.7461\n",
      "Training model with 32 layers, 164 units per layer, and learning rate 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lior\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:868: FutureWarning:\n",
      "\n",
      "`sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1012/1012 [==============================] - 90s 85ms/step - loss: 281.2530 - mse: 281.2530 - val_loss: 5.8312 - val_mse: 5.8312\n",
      "Epoch 2/10\n",
      "1012/1012 [==============================] - 90s 89ms/step - loss: 5.8566 - mse: 5.8566 - val_loss: 5.9373 - val_mse: 5.9373\n",
      "Epoch 3/10\n",
      "1012/1012 [==============================] - 94s 92ms/step - loss: 5.8946 - mse: 5.8946 - val_loss: 5.8917 - val_mse: 5.8917\n",
      "Epoch 4/10\n",
      "1012/1012 [==============================] - 100s 99ms/step - loss: 5.9632 - mse: 5.9632 - val_loss: 5.8195 - val_mse: 5.8195\n",
      "Epoch 5/10\n",
      "1012/1012 [==============================] - 96s 95ms/step - loss: 5.9857 - mse: 5.9857 - val_loss: 6.3758 - val_mse: 6.3758\n",
      "Epoch 6/10\n",
      "1012/1012 [==============================] - 87s 86ms/step - loss: 6.0533 - mse: 6.0533 - val_loss: 6.1915 - val_mse: 6.1915\n",
      "Epoch 7/10\n",
      "1012/1012 [==============================] - 87s 86ms/step - loss: 6.0673 - mse: 6.0673 - val_loss: 5.8530 - val_mse: 5.8530\n",
      "Epoch 8/10\n",
      "1012/1012 [==============================] - 86s 85ms/step - loss: 6.0854 - mse: 6.0854 - val_loss: 6.0289 - val_mse: 6.0289\n",
      "Epoch 9/10\n",
      "1012/1012 [==============================] - 85s 84ms/step - loss: 6.0140 - mse: 6.0140 - val_loss: 5.8532 - val_mse: 5.8532\n",
      "Epoch 10/10\n",
      "1012/1012 [==============================] - 84s 83ms/step - loss: 5.9985 - mse: 5.9985 - val_loss: 5.8881 - val_mse: 5.8881\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'best_params' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 118\u001b[0m\n\u001b[0;32m    114\u001b[0m                 best_score \u001b[38;5;241m=\u001b[39m mse\n\u001b[0;32m    115\u001b[0m                 best_model \u001b[38;5;241m=\u001b[39m model\n\u001b[1;32m--> 118\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest Model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_params\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    119\u001b[0m predictions \u001b[38;5;241m=\u001b[39m best_model\u001b[38;5;241m.\u001b[39mpredict([X_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdescription\u001b[39m\u001b[38;5;124m'\u001b[39m], X_test_transformed])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'best_params' is not defined"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Concatenate\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Load your dataset\n",
    "wine_reviews = pd.read_csv(\"clean_wine_reviews_data.csv\")\n",
    "\n",
    "            \n",
    "# Preprocessing\n",
    "wine_reviews = wine_reviews.drop(['Unnamed: 0', 'taster_twitter_handle', 'region_1', 'region_2', 'designation'], axis=1)\n",
    "wine_reviews['description_length'] = wine_reviews.description.str.len()\n",
    "wine_reviews = wine_reviews.dropna(subset=['year', 'points'])\n",
    "wine_reviews['year'] = wine_reviews['year'].astype(int).astype(str)\n",
    "wine_reviews[\"description_length\"] = wine_reviews[\"description_length\"].astype(np.int32)\n",
    "\n",
    "# Split data\n",
    "y = wine_reviews['points']\n",
    "X = wine_reviews.drop(['points'], axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=29)\n",
    "\n",
    "# Define a range of hyperparameters to search over\n",
    "first_layer_list = [16, 32]\n",
    "second_layer_list = [8, 16]\n",
    "learning_rate_list = [0.001, 0.01]\n",
    "\n",
    "best_model = None\n",
    "best_score = float('inf')\n",
    "\n",
    "for first_layer in first_layer_list:\n",
    "    for second_layer in second_layer_list:\n",
    "        for lrn_rate in learning_rate_list:\n",
    "            print(f\"Training model with {first_layer} layers, {second_layer} units per layer, and learning rate {lrn_rate}\")\n",
    "\n",
    "\n",
    "            # Preprocess categorical data\n",
    "            categorical_cols = ['country', 'taster_name', 'variety', 'province', 'year']\n",
    "            numeric_cols = ['price']\n",
    "\n",
    "            # Use StandardScaler for numeric columns\n",
    "            numeric_transformer = StandardScaler()\n",
    "\n",
    "            # Use OneHotEncoder for categorical columns\n",
    "            categorical_transformer = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    "\n",
    "            # Preprocess features using ColumnTransformer\n",
    "            from sklearn.compose import ColumnTransformer\n",
    "\n",
    "            preprocessor = ColumnTransformer(\n",
    "                transformers=[\n",
    "                    ('num', numeric_transformer, numeric_cols),\n",
    "                    ('cat', categorical_transformer, categorical_cols)\n",
    "                ])\n",
    "\n",
    "            # Fit and transform preprocessing on training data\n",
    "            X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "            X_test_transformed = preprocessor.transform(X_test)\n",
    "\n",
    "            # Text Vectorization\n",
    "            max_vocab_length = 10000\n",
    "            max_length = 60\n",
    "\n",
    "            text_vectorizer = tf.keras.layers.TextVectorization(\n",
    "                max_tokens=max_vocab_length,\n",
    "                output_mode=\"int\",\n",
    "                output_sequence_length=max_length)\n",
    "\n",
    "            text_vectorizer.adapt(X_train['description'])\n",
    "            text_input = Input(shape=(1,), dtype=\"string\")\n",
    "            text_vector = text_vectorizer(text_input)\n",
    "            text_embedding = Embedding(\n",
    "                input_dim=max_vocab_length,\n",
    "                output_dim=128,\n",
    "                embeddings_initializer=\"uniform\",\n",
    "                input_length=max_length)(text_vector)\n",
    "            text_lstm = LSTM(64)(text_embedding)\n",
    "\n",
    "            # Combine numeric and categorical data\n",
    "            combined_input = Input(shape=(X_train_transformed.shape[1],), dtype=\"float32\")\n",
    "            concatenated = Concatenate()([text_lstm, combined_input])\n",
    "\n",
    "            # Build the model\n",
    "            x = Dense(first_layer, activation='relu')(concatenated)\n",
    "            x = Dense(second_layer, activation='relu')(x)\n",
    "            output = Dense(1, activation=\"linear\")(x)\n",
    "            \n",
    "    \n",
    "            # Define the learning rate variable\n",
    "            learning_rate = tf.Variable(lrn_rate, trainable=False)\n",
    "            tf.keras.backend.set_value(learning_rate, lrn_rate)\n",
    "\n",
    "            model = Model(inputs=[text_input, combined_input], outputs=output)\n",
    "\n",
    "            # Compile the model\n",
    "            model.compile(loss=tf.keras.losses.MeanSquaredError(),\n",
    "                          optimizer=tf.keras.optimizers.Adam(),\n",
    "                          metrics=['mse'])\n",
    "\n",
    "            # Train the model\n",
    "            model.fit([X_train['description'], X_train_transformed], y_train,\n",
    "                      batch_size=64,\n",
    "                      epochs=10,\n",
    "                      validation_split=0.2) \n",
    "\n",
    "\n",
    "            loss, mse = model.evaluate([X_test['description'], X_test_transformed], y_test, verbose=0)\n",
    "\n",
    "            # Check if this is the best model so far\n",
    "            if mse < best_score:\n",
    "                best_score = mse\n",
    "                best_model = model\n",
    "                \n",
    "\n",
    "predictions = best_model.predict([X_test['description'], X_test_transformed])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "843/843 [==============================] - 17s 20ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = best_model.predict([X_test['description'], X_test_transformed])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3622686397744588\n"
     ]
    }
   ],
   "source": [
    "preds = tf.squeeze(tf.round(predictions))\n",
    "print(r2_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5057/5057 [==============================] - 216s 42ms/step - loss: 5.5139 - accuracy: 0.0000e+00 - mean_squared_error: 0.0000e+00\n",
      "Epoch 2/10\n",
      "5057/5057 [==============================] - 208s 41ms/step - loss: 5.5124 - accuracy: 1.2359e-05 - mean_squared_error: 0.0000e+00\n",
      "Epoch 3/10\n",
      "5057/5057 [==============================] - 210s 42ms/step - loss: 5.4933 - accuracy: 0.0000e+00 - mean_squared_error: 0.0000e+00\n",
      "Epoch 4/10\n",
      "1740/5057 [=========>....................] - ETA: 2:18 - loss: 5.4747 - accuracy: 0.0000e+00 - mean_squared_error: 0.0000e+00"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.metrics import Accuracy\n",
    "from tensorflow.keras.metrics import MeanSquaredError\n",
    "early_stopping_monitor = EarlyStopping(\n",
    "    monitor='mean_squared_error',\n",
    "    min_delta=0.01,\n",
    "    patience=3,\n",
    "    verbose=0,\n",
    "    mode='min',\n",
    "    baseline=None,\n",
    "    restore_best_weights=True\n",
    ") \n",
    "\n",
    "model.compile(optimizer=Adam(),\n",
    "              loss='mean_squared_error',\n",
    "              metrics=Accuracy())\n",
    "# Train the model\n",
    "model.fit( [X_train['description'], X_train_transformed], y_train,\n",
    "          batch_size=16,\n",
    "          epochs=10,\n",
    "          callbacks=[early_stopping_monitor])\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = model.evaluate([X_test['description'], X_test_transformed], y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict([X_test['description'], X_test_transformed])\n",
    "preds = tf.squeeze(tf.round(predictions))\n",
    "print(r2_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loss, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

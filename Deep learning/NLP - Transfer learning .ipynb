{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90fb7fcb-1bd2-4bda-97a6-6816750fb37a",
   "metadata": {},
   "source": [
    "# Transfer learning using Transformers "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849f4ccc-fec3-4e6f-b1a5-06e198d772e5",
   "metadata": {},
   "source": [
    "## Using transformers for feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a35770d-97fb-4d33-8fb5-6474ae5dbdfa",
   "metadata": {
    "tags": []
   },
   "source": [
    "Hugging Face is a company that provides a wide range of pre-trained models and tools for natural language processing tasks. They have a library named transformers which provides thousands of pre-trained models to perform tasks on texts such as classification, information extraction, summarization, translation, text generation, and more. The library is built with an aim to make state-of-the-art NLP accessible to everyone and is widely used in the community.\n",
    "\n",
    "**Finding Other Hugging Face Models**:\n",
    "You can find all pre-trained models provided by Hugging Face on their [model hub](https://huggingface.co/models). You can filter these models by task, language, and more. For each model, you will find usage details, including how to load the model using the transformers library.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf2e650-dc19-45bf-b8ee-6ac465dd7e7f",
   "metadata": {},
   "source": [
    "\n",
    "1. **CLS and SEP Tokens**:\n",
    "In BERT-style models (including DistilBert), special tokens are used to provide boundary and classification information. The \"[CLS]\" token is a special token added at the beginning of each sentence, which is used as an aggregate representation for classification tasks. The \"[SEP]\" token is used to denote separation between sentences, particularly in tasks that require understanding of two sentences (like question answering or natural language inference).\n",
    "\n",
    "2. **Sentence Representation**:\n",
    "While simple averaging of word vectors to create sentence vectors is a common approach, it may not always be the best. For models like BERT and DistilBert, the embedding corresponding to the \"[CLS]\" token is often used as the sentence representation. This is especially true if the model has been fine-tuned on a task similar to the one you are working on, as the fine-tuning process will modify the model so that the \"[CLS]\" token captures relevant information about the sentence as a whole.\n",
    "\n",
    "3. **DistilBert and Alternatives**:\n",
    "DistilBert is a lighter and faster version of BERT. It retains 95% of BERTâ€™s performance while being 60% smaller and 60% faster. It achieves this by removing the token-type embeddings and the pooler (used for next sentence prediction tasks), and also by training the model to mimic BERT's behavior. \n",
    "\n",
    "    Alternatives to DistilBert include:\n",
    "    * BERT: The original model, which has multiple versions of varying sizes.\n",
    "    * RoBERTa: A version of BERT that uses dynamic masking rather than static masking.\n",
    "    * ALBERT: A lite BERT that reuses parameters across layers, resulting in significant reduction in size.\n",
    "    * XLNet: A generalized autoregressive model that outperforms BERT on several benchmarks.\n",
    "    * GPT-2 and GPT-3: Transformer models designed for language generation tasks.\n",
    "\n",
    "    You can use these alternatives in a similar way to DistilBert, just replace 'distilbert-base-uncased' with the model name you want (like 'bert-base-uncased', 'roberta-base', 'albert-base-v2', 'xlnet-base-cased', 'gpt2', etc.) in the code provided earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9928888a-e7a9-4e43-9f53-38f180f953e3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/idoda/anaconda3/envs/python_ml_2023/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertModel, RobertaModel, RobertaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04022801-8b62-4fab-9a03-06cb2497458b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained model and tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dff738b9-103d-4b1e-bfcb-2662d3353f16",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# For roberta model, for example:\n",
    "# Load the pre-trained RoBERTa model\n",
    "roberta_model = RobertaModel.from_pretrained('roberta-base')\n",
    "\n",
    "# Load the tokenizer associated with the model\n",
    "roberta_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63435be5-cbed-458a-a1d3-63dba6243a3f",
   "metadata": {},
   "source": [
    "\n",
    "To find out the available model names for RoBERTa or any other model in the Hugging Face's Transformers library, you can refer to the official Hugging Face Model Hub documentation. The Model Hub provides a wide range of pre-trained models that you can use.\n",
    "\n",
    "You can visit the Hugging Face Model Hub website at https://huggingface.co/models and search for RoBERTa models specifically. H\n",
    "For example, some common RoBERTa models you might find are:\n",
    "\n",
    "* roberta-base: The base RoBERTa model with 12 layers and 110 million parameters.\n",
    "* roberta-large: A larger RoBERTa model with 24 layers and 355 million parameters.\n",
    "* roberta-large-mnli: A RoBERTa model pre-trained on the MultiNLI dataset.\n",
    "\n",
    "These names can be passed to the from_pretrained method to load the corresponding RoBERTa models.\n",
    "Remember to consult the documentation and the model's README file for more information on each model, including the input/output formats, fine-tuning tasks, and any specific instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf5f2cb-a4e0-440a-9664-796bae525ff5",
   "metadata": {},
   "source": [
    "**I will use the distilled Bert model for the next demonstration, but the concept is the same with the rest of the BERT family models.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57e3cf2c-dcdb-45b9-b168-d575cc3968b3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-5.75909903e-03 -1.40274286e-01  6.08086586e-04 -1.64577499e-01\n",
      "  2.39165109e-02 -3.49079072e-01  1.14989005e-01  3.25672895e-01\n",
      "  3.41304064e-01 -2.07848027e-01 -3.44636321e-01 -1.31399527e-01\n",
      " -2.10214600e-01  1.57644272e-01 -2.64885396e-01  3.34598899e-01\n",
      " -3.45788933e-02  1.43658146e-01  5.99903241e-03 -1.34861842e-01\n",
      "  1.15781657e-01  3.37876678e-01 -2.10142940e-01  2.18721047e-01\n",
      "  6.12023413e-01 -2.36835033e-01  3.14411938e-01 -2.31933132e-01\n",
      " -3.67941141e-01  3.29555646e-02  2.74281979e-01  2.93108165e-01\n",
      " -2.37404536e-02 -2.98157543e-01  1.70525998e-01  6.03910461e-02\n",
      "  5.33379316e-01 -2.53717005e-01 -4.15424742e-02  1.16764858e-01\n",
      " -4.99883920e-01 -1.38034701e-01  3.69493395e-01  9.91934240e-02\n",
      "  5.66613339e-02 -3.94730270e-01  1.38495211e-02 -2.30717529e-02\n",
      " -7.10175037e-02 -7.83770531e-03]\n",
      "CPU times: user 30.1 ms, sys: 14.8 ms, total: 44.8 ms\n",
      "Wall time: 33.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# Text to vectorize\n",
    "text = \"Here is an example paragraph that we will convert into an embedding.\"\n",
    "\n",
    "# Add special tokens for BERT (start and end of sentence)\n",
    "marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "\n",
    "# Tokenize our sentence\n",
    "tokenized_text = tokenizer.tokenize(marked_text)\n",
    "\n",
    "# Map tokens to their index in the tokenizer vocabulary\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "# Convert list to torch tensor\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "\n",
    "# Put everything on the GPU if available and run through the model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "tokens_tensor = tokens_tensor.to(device)\n",
    "model = model.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(tokens_tensor)\n",
    "    # The first element of outputs is the last layer of the model, which can be used as embeddings.\n",
    "    embeddings = outputs[0]\n",
    "\n",
    "# Calculate the mean to get sentence vector\n",
    "mean_embeddings = torch.mean(embeddings, dim=1).cpu().numpy()\n",
    "\n",
    "print(mean_embeddings.flatten()[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e0f6d31-61bc-4ce3-a789-65fa1d8768b3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'here',\n",
       " 'is',\n",
       " 'an',\n",
       " 'example',\n",
       " 'paragraph',\n",
       " 'that',\n",
       " 'we',\n",
       " 'will',\n",
       " 'convert',\n",
       " 'into',\n",
       " 'an',\n",
       " 'em',\n",
       " '##bed',\n",
       " '##ding',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2aadf24c-1844-4a10-a88a-e63c6cdaef75",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 17, 768])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1dec8c4a-3622-443b-a7c6-8c6b6852a916",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'here', 'is', 'an', 'example', 'paragraph', 'that', 'we', 'will', 'convert', 'into', 'an', 'em', '##bed', '##ding', '.', '[SEP]']\n",
      "17\n",
      "torch.Size([1, 17, 768])\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_text)\n",
    "print(len(tokenized_text))\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5112e10f-afd0-43a8-bf07-73889f1387c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768,)\n"
     ]
    }
   ],
   "source": [
    "print(mean_embeddings.flatten().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0702b4-494c-468f-97c4-42d75f9bf825",
   "metadata": {},
   "source": [
    "### Using GPT models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fd525fb6-f5dc-4c6c-936b-7f083ac18508",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2Model.from_pretrained('gpt2')\n",
    "\n",
    "# Text to vectorize\n",
    "text = \"Here is an example paragraph that we will convert into an embedding.\"\n",
    "\n",
    "# Tokenize our sentence\n",
    "input_ids = tokenizer.encode(text, return_tensors='pt')\n",
    "\n",
    "# Run through the model\n",
    "outputs = model(input_ids)\n",
    "# The first element of outputs is the last layer of the model, which can be used as embeddings.\n",
    "embeddings = outputs[0]\n",
    "\n",
    "# Calculate the mean to get sentence vector\n",
    "mean_embeddings = torch.mean(embeddings, dim=1).cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "24df0e9a-4b84-4603-b010-87b93ab970ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6.82176873e-02 -2.17835337e-01 -7.27493167e-01  2.93786347e-01\n",
      " -1.58162013e-01 -5.51667623e-02  2.28856421e+00  1.47566915e-01\n",
      "  2.66588796e-02 -3.06096047e-01  1.96142383e-02  6.88604340e-02\n",
      " -2.73834676e-01 -3.17501999e-03 -6.38329834e-02  1.09410897e-01\n",
      " -2.67304450e-01  7.81244412e-02  3.33967358e-01 -4.50377464e-01\n",
      "  6.72684684e-02  1.86494902e-01  8.82443041e-02 -1.19551606e-01\n",
      "  5.64160869e-02 -1.86481867e-02 -5.05188107e-01 -3.79158318e-01\n",
      "  2.73496896e-01  9.72704440e-02 -2.41519675e-01 -2.06735775e-01\n",
      "  4.31770161e-02 -1.41417785e-02  1.27175167e-01  1.24921359e-01\n",
      "  6.93927994e+01  4.60944235e-01 -2.53891051e-01  2.88569361e-01\n",
      "  2.90439814e-01  4.95970100e-01  7.61573985e-02  5.72054163e-02\n",
      " -2.42845088e-01  1.03123374e-01 -2.68422186e-01 -5.21096289e-02\n",
      " -3.16923827e-01  1.00429356e+00]\n"
     ]
    }
   ],
   "source": [
    "print(mean_embeddings.flatten()[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02f2fd2-d8e7-4b7f-aafc-12bb5f0ca2dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_ml_2023",
   "language": "python",
   "name": "python_ml_2023"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
